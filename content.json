{"meta":{"title":"Grooes.com","subtitle":"The power of expression","description":"The power of expression","author":"qinghe Song","url":"http://grooes.com"},"pages":[{"title":"categories","date":"2017-09-17T02:02:50.000Z","updated":"2017-09-17T02:03:03.349Z","comments":false,"path":"categories/index.html","permalink":"http://grooes.com/categories/index.html","excerpt":"","text":""},{"title":"search","date":"2017-09-17T03:54:42.718Z","updated":"2017-09-17T03:54:42.718Z","comments":false,"path":"search/index.html","permalink":"http://grooes.com/search/index.html","excerpt":"","text":""},{"title":"link","date":"2017-09-17T03:38:42.000Z","updated":"2017-09-17T03:39:33.549Z","comments":true,"path":"link/index.html","link":[{"name":"����������","info":"���������꣬����һ����·��","url":"http://yuyinghua.com/","avatar":"http://yuyinghua.com/icon.png"}],"permalink":"http://grooes.com/link/index.html","excerpt":"","text":""},{"title":"tags","date":"2017-09-19T12:50:17.000Z","updated":"2017-09-20T12:56:22.282Z","comments":true,"path":"tags/index.html","permalink":"http://grooes.com/tags/index.html","excerpt":"","text":"tags: php go algorithm mysql linux interview"}],"posts":[{"title":"算法那点事儿","slug":"算法那点事儿","date":"2017-11-05T14:20:40.000Z","updated":"2017-11-05T14:26:47.970Z","comments":true,"path":"2017/11/05/算法那点事儿/","link":"","permalink":"http://grooes.com/2017/11/05/算法那点事儿/","excerpt":"","text":"算法题那点事儿 连续1到n的数字缺少x，乱序存在一个数组中，找出这个x是多少 1234567891011//异或有个很巧妙的地方：同一变量和该变量与另一变量的异或值的异或等于这个变量自身，最后计算下来时间复杂度为O(n),空间复杂度为O（1）$a = [1,2,3,4,8,6,7,9,10,11,12];$x = 0;$y = 0;for($i=1; $i&lt;=12; $i++) &#123; $y^=$i;&#125;foreach($a as $i) &#123; $x^=$i;&#125;echo $x^$y;//5","categories":[],"tags":[{"name":"php","slug":"php","permalink":"http://grooes.com/tags/php/"},{"name":"interview","slug":"interview","permalink":"http://grooes.com/tags/interview/"},{"name":"algorithm","slug":"algorithm","permalink":"http://grooes.com/tags/algorithm/"}]},{"title":"redis之复制篇","slug":"redis之复制篇","date":"2017-10-17T13:26:02.000Z","updated":"2017-10-17T13:26:26.907Z","comments":true,"path":"2017/10/17/redis之复制篇/","link":"","permalink":"http://grooes.com/2017/10/17/redis之复制篇/","excerpt":"","text":"redis设计与实现之复制篇当客户端向从服务器发送 SLAVEOF 命令， 要求从服务器复制主服务器时， 从服务器首先需要执行同步操作， 也即是， 将从服务器的数据库状态更新至主服务器当前所处的数据库状态（以2.8版本为分水岭进行对新旧复制功能的讲解）12#以下命令就能得知12345为从服务器，6379为主服务器127.0.0.1:12345&gt; slaveof 127.0.0.1 6379 旧版复制功能的实现Redis 的复制功能分为同步（sync）和命令传播（command propagate）两个操作 同步操作用于将从服务器的数据库状态更新至主服务器当前所处的数据库状态 命令传播操作则用于在主服务器的数据库状态被修改， 导致主从服务器的数据库状态出现不一致时， 让主从服务器的数据库重新回到一致状态 同步如下图展示了同步的过程 大体流程为 从服务器向主服务器发送 SYNC 命令 收到 SYNC 命令的主服务器执行 BGSAVE 命令， 在后台生成一个 RDB 文件， 并使用一个缓冲区记录从现在开始执行的所有写命令 当主服务器的 BGSAVE 命令执行完毕时， 主服务器会将 BGSAVE 命令生成的 RDB 文件发送给从服务器， 从服务器接收并载入这个 RDB 文件， 将自己的数据库状态更新至主服务器执行 BGSAVE 命令时的数据库状态 主服务器将记录在缓冲区里面的所有写命令发送给从服务器， 从服务器执行这些写命令， 将自己的数据库状态更新至主服务器数据库当前所处的状态 命令传播主从处于一直状态下的结构图 这是如果客户端向主服务器发送命令 DEL k3，如下图造成不一致状态 这时候需要经过命令传播把命令发送给从服务，从而达到一致状态 旧版复制功能的缺陷从上面的流程可以看出，Slave从库在连接Master主库时，Master会进行内存快照，然后把整个快照文件发给Slave，也就是没有象MySQL那样有复制位置的概念，即无增量复制，这会给整个集群搭建带来非常多的问题。 比如一台线上正在运行的Master主库配置了一台从库进行简单读写分离，这时Slave由于网络或者其它原因与Master断开了连接，那么当 Slave进行重新连接时，需要重新获取整个Master的内存快照，Slave所有数据跟着全部清除，然后重新建立整个内存表，一方面Slave恢复的 时间会非常慢，另一方面也会给主库带来压力 sync命令是一个非常耗费资源的操作，每次执行sync命令主服务器都会执行以下动作 主服务器只需要执行bgsave来生成rdb文件，这个生成操作会耗费主服务器大量的cpu、内存和磁盘IO资源 主需要将rdb文件发送给从，发送操作会耗费主从服务器大量的网络资源(带宽和流量)，并对主服务器相应命令请求的时间产生影响 接收到rdb文件的从需要载入主服务器发来的rdb文件，并且在载入期间会因为阻塞而没办法处理命令请求 新版复制功能的实现从2.8版本开始，用psync命令（完整重同步和部分重同步两种模式）代替sync命令来执行复制时的同步操作完整重同步没啥好说的，和sync差不过部分重同步结构图如下 部分重同步的实现部分重同步功能由以下三部分构成 主服务器的复制偏移量和从服务器的复制偏移量 主服务器的复制积压缓冲区 服务器运行ID 复制偏移量主从都会维护一个偏移量 master每次向slave传播N字节，会将其偏移量+N slave每次接受master的N字节，会将其偏移量+N 如下图（通过主从的复制偏移量很容易得知主从是否处于一致状态） 考虑下图的一种情况 如果从服务器A断线重连，会执行哪个完整还是部分？如果执行部分，主是如何补偿从A这部分断线期间丢失的数据？这个问题的答案和复制积压缓冲区有关 复制积压缓冲区复制积压缓冲区是由master维护的一个固定长度先进先出队列（和经常说的队列一样），默认大小位1M，当master进行命令传播时，他不仅会将写命令发给所有slave，还会将写命令入队到复制积压缓冲区里面，如下图所示 复制积压缓冲区结构图如下（会带有偏移量） 当slave重连master时，slave会通过psync经offset发给master，master会根据offset来决定对slave执行何种同步操作 offset之后的数据仍然存在于积压缓冲区里面，master会执行部分重同步操作 offset之后的数据不存在于积压缓冲区里面，master会执行完成重同步操作 复制积压缓冲区的最小大小 = second（断线重连所需要的时间） write_size_per_second（每秒产生的写命令数据量）保险起见可以 2，可以保证绝大部分断线情况都能用部分重同步来处理，缓冲区大小的设置可查看repl_backlog_size来配置 服务器运行IDredis都会有一个自己运行ID（由40个随机的十六进制字符组成），初次复制，master会将自己运行ID传送给slave，slave会将这个ID保存起来，断线重连时会将此ID传送给master，若master ID = slave ID 则执行部分重同步，否则则执行完整重同步 psync命令的实现psync命令的调用方法有两种 slave从未复制过master或者之前执行过slaveof no one命令，那么slave会向master发送psync ? -1 进行完整重同步 相反，会执行psync runid offset (runid是上一次复制的master的运行ID，offset则是slave当前的复制偏移量) psync中master向slave返回以下三种回复 若master返回 + fullresync runid offset,表示执行了完整重同步，runid是master运行ID，slave会把这个id保存起来，offset为master的偏移量，slave也会保存起来作为自己的偏移量 若master返回 + continue，表示执行了部分重同步，从只需要等着master将自己缺少的那部分数据发送过来即可 若master返回 -err，则表示redis版本低于2.8，识别不了psync命令 如下图所示 复制的实现设置主服务器的地址和端口12345678910# 执行命令127.0.0.1:12345&gt; slaveof 127.0.0.1 6379# slave服务器需要将127.0.0.1保存到masterhost中，6379保存到masterport中struct redisServer &#123; # 主服务器的地址 char *masterhost; # 主服务器的端口 char *masterport;&#125; 简历套接字连接如下图所示 发送PING命令如下图所示 ping命令的作用 检查套接字读写是否正常 检查master能够正常处理命令请求 ping命令可能会遇到的问题如下图所示 身份验证slave收到master的pong之后就会验证身份 如果slave设置了masterauth，那么进行身份验证（slave会向master发送 auth masterauth） 如果slave未设置masterauth，那么不进行身份验证 如下图所示 发送端口信息身份验证完事儿之后，slave会执行replconf listening-port port-number,向master发送slave的监听端口号（case:12345）master接收到这个port后，会将port记录在slave服务器所对应的客户端状态的slave_listening_port属性中123typedef struct redisClient &#123; int slave_listening_port;&#125; 同步slave将向master发送psync命令，如下图所示 命令传播完成同步之后，master进入命令传播阶段，会一直将写命令发送给slave即可 心跳检测命令传播阶段，slave默认以1s的频率，向master发送命令replconf ack replication_offset(slave当前的复制偏移量)发送replconf ack的三个作用 检查master-slave网络连接状态 辅助实现min-slaves选项 检测命令丢失 检测master的网络状态如果1s还未返回任何信息，就知道连接出现问题了，在master上执行info replication命令，可以查看明细 检测命令丢失如下图所示，通过master的偏移量和slave的偏移量得知那部分命令丢失了，并且master会主动set key value至slave，redis2.8版本之前的即使命令丢失也不知道","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://grooes.com/tags/redis/"}]},{"title":"redis之慢查询篇","slug":"redis之慢查询篇","date":"2017-10-16T12:57:15.000Z","updated":"2017-10-16T12:57:56.943Z","comments":true,"path":"2017/10/16/redis之慢查询篇/","link":"","permalink":"http://grooes.com/2017/10/16/redis之慢查询篇/","excerpt":"","text":"redis设计与实现之慢查询篇服务器配置有两个和慢查询日志相关的选项： slowlog-log-slower-than 选项指定执行时间超过多少微秒（1s = 1,000,000μs）的命令请求会被记录到日志上 slowlog-max-len 选项指定服务器最多保存多少条慢查询日志（先进先出的方式保存多条慢查询日志）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485//CONFIG_SET 命令将 slowlog-log-slower-than 选项的值设为 0 微秒redis&gt; CONFIG SET slowlog-log-slower-than 0OK//slowlog-max-len 选项的值设为 5 ， 让服务器最多只保存 5 条慢查询日志redis&gt; CONFIG SET slowlog-max-len 5OK//紧接着执行以下命令redis&gt; SET msg \"hello world\"OKredis&gt; SET number 10086OKredis&gt; SET database \"Redis\"OK//SLOWLOG GET 命令查看服务器所保存的慢查询日志redis&gt; SLOWLOG GET1) 1) (integer) 4 # 日志的唯一标识符（uid） 2) (integer) 1378781447 #命令执行时的时间戳 3) (integer) 13 # 命令执行的时长，以微秒计算 4) 1) \"SET\" # 命令以及命令参数 2) \"database\" 3) \"Redis\"2) 1) (integer) 3 2) (integer) 1378781439 3) (integer) 10 4) 1) \"SET\" 2) \"number\" 3) \"10086\"3) 1) (integer) 2 2) (integer) 1378781436 3) (integer) 18 4) 1) \"SET\" 2) \"msg\" 3) \"hello world\"4) 1) (integer) 1 2) (integer) 1378781425 3) (integer) 11 4) 1) \"CONFIG\" 2) \"SET\" 3) \"slowlog-max-len\" 4) \"5\"5) 1) (integer) 0 2) (integer) 1378781415 3) (integer) 53 4) 1) \"CONFIG\" 2) \"SET\" 3) \"slowlog-log-slower-than\" 4) \"0\"//在执行SLOWLOG GET 可以看到已经先进先出了redis&gt; SLOWLOG GET1) 1) (integer) 5 2) (integer) 1378781521 3) (integer) 61 4) 1) \"SLOWLOG\" 2) \"GET\"2) 1) (integer) 4 2) (integer) 1378781447 3) (integer) 13 4) 1) \"SET\" 2) \"database\" 3) \"Redis\"3) 1) (integer) 3 2) (integer) 1378781439 3) (integer) 10 4) 1) \"SET\" 2) \"number\" 3) \"10086\"4) 1) (integer) 2 2) (integer) 1378781436 3) (integer) 18 4) 1) \"SET\" 2) \"msg\" 3) \"hello world\"5) 1) (integer) 1 2) (integer) 1378781425 3) (integer) 11 4) 1) \"CONFIG\" 2) \"SET\" 3) \"slowlog-max-len\" 4) \"5\" 慢查询记录保存1234567891011121314struct redisServer &#123; // 下一条慢查询日志的 ID，初始值为 0，新增慢查询则累加 long long slowlog_entry_id; // 保存了所有慢查询日志的链表,每个节点都保存了一个 slowlogEntry 结构 list *slowlog; // 服务器配置 slowlog-log-slower-than 选项的值 long long slowlog_log_slower_than; // 服务器配置 slowlog-max-len 选项的值 unsigned long slowlog_max_len;&#125;; 12345678910111213141516typedef struct slowlogEntry &#123; // 唯一标识符 long long id; // 命令执行时的时间，格式为 UNIX 时间戳 time_t time; // 执行命令消耗的时间，以微秒为单位 long long duration; // 命令与命令参数 robj **argv; // 命令与命令参数的数量 int argc;&#125; slowlogEntry; slowlogEntry结构图如下（最新的排在表头，反之表尾） 慢查询日志的阅览和删除SLOWLOG GET 命令实现如下 12345678910111213141516171819def SLOWLOG_GET(number=None): # 用户没有给定 number 参数 # 那么打印服务器包含的全部慢查询日志 if number is None: number = SLOWLOG_LEN() # 遍历服务器中的慢查询日志 for log in redisServer.slowlog: if number &lt;= 0: # 打印的日志数量已经足够，跳出循环 break else: # 继续打印，将计数器的值减一 number -= 1 # 打印日志 printLog(log) SLOWLOG LEN命令实现如下1234def SLOWLOG_LEN(): # slowlog 链表的长度就是慢查询日志的条目数量 return len(redisServer.slowlog) SLOWLOG RESET 命令实现如下1234567def SLOWLOG_RESET(): # 遍历服务器中的所有慢查询日志 for log in redisServer.slowlog: # 删除日志 deleteLog(log) 添加新日志在每次执行命令的之前和之后， 程序都会记录微秒格式的当前 UNIX 时间戳， 这两个时间戳之间的差就是服务器执行命令所耗费的时长， 服务器会将这个时长作为参数之一传给 slowlogPushEntryIfNeeded 函数， 而 slowlogPushEntryIfNeeded 函数则负责检查是否需要为这次执行的命令创建慢查询日志， 以下伪代码展示了这一过程1234567891011# 记录执行命令前的时间before = unixtime_now_in_us()# 执行命令execute_command(argv, argc, client)# 记录执行命令后的时间after = unixtime_now_in_us()# 检查是否需要创建新的慢查询日志slowlogPushEntryIfNeeded(argv, argc, before-after) slowlogPushEntryIfNeeded 有两个作用 检查命令的执行时长是否超过 slowlog-log-slower-than 选项所设置的时间， 如果是的话， 就为命令创建一个新的日志， 并将新日志添加到 slowlog 链表的表头 检查慢查询日志的长度是否超过 slowlog-max-len 选项所设置的长度， 如果是的话， 那么将多出来的日志从 slowlog 链表中删除掉1234567891011121314void slowlogPushEntryIfNeeded(robj **argv, int argc, long long duration) &#123; // 慢查询功能未开启，直接返回 if (server.slowlog_log_slower_than &lt; 0) return; // 如果执行时间超过服务器设置的上限，那么将命令添加到慢查询日志 if (duration &gt;= server.slowlog_log_slower_than) // 新日志添加到链表表头 listAddNodeHead(server.slowlog,slowlogCreateEntry(argv,argc,duration)); // 如果日志数量过多，那么进行删除 while (listLength(server.slowlog) &gt; server.slowlog_max_len) listDelNode(server.slowlog,listLast(server.slowlog));&#125;","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://grooes.com/tags/redis/"}]},{"title":"redis之持久化篇","slug":"redis之持久化篇","date":"2017-10-10T13:33:04.000Z","updated":"2017-10-10T13:39:29.662Z","comments":true,"path":"2017/10/10/redis之持久化篇/","link":"","permalink":"http://grooes.com/2017/10/10/redis之持久化篇/","excerpt":"","text":"redis设计与实现之持久篇Redis有两种持久化方式:RDB持久化和AOF持久化，前者是通过把数据库状态的键值对保存起来，而AOF则是把命令保存起来,那么服务器是使用哪种持久化呢？因为AOF文件更新频率更高，所以优先AOF，接下来将分别介绍两种持久化方式 RDB持久化服务器可以通过save和bgsave来实现rdb持久化,前者会阻塞服务器进程，导致在进行save操作期间,服务器无法继续处理客户端请求;bgsave则会派生一个子进程，让子进程去进行持久化操作，而服务器进程继续响应客户端，这里要说明的是bgsave和save这两个操作无法同时进行,因为会出现竞争条件，由于bgsave会创建子进程来进行持久化操作,并不阻碍服务器进程处理其他事,所以我们可以让服务器每隔一段时间执行一次bgsave,因此可以通过设置保存条件来让服务器执行bgsave操作12345678910struct redisServer&#123; struct saveparam *saveparam;//记录了保存条件的数组 sds aof_buf //AOF缓冲区,用来存放写命令的协议请求内容&#125;;//条件:当满足条件(在time_t秒内,修改次数达到changes)时,服务器执行bgsavestruct saveparam&#123; time_t seconds;//秒数 int changes//修改数&#125;; Redis服务器有个周期性函数serverCron,默认每隔100ms执行一次,Redis就是通过它来检测保存条件12345678def serverCron(): //遍历所有的保存条件 for saveparam in serverparams: //计算距离上次执行保存操作有多少秒 save_interval = unixtime_now() - server.lastsave //如果时间和次数都符合则执行bgsave if server.dirty &gt;= saveparam.changes and save_interval &gt; saveparam.seconds: BGSAVE() RDB文件结构如下 REDIS长度5字节,保存着’R’ ‘E’ ‘D’ ‘I’ ‘S’ 五个字符,可用来判断文件是否是rdb文件（RDB 文件保存的是二进制数据， 而不是 C 字符串） db_version长度为4字节,值为整数型字符串,代表rdb文件的版本号.比如0006 databases 数据库状态,包含一个或多个数据库,以及各个数据库中的键值对（1、如果服务器的数据库状态为空（所有数据库都是空的）， 那么这个部分也为空， 长度为 0 字节。2、如果服务器的数据库状态为非空（有至少一个数据库非空）， 那么这个部分也为非空， 根据数据库所保存键值对的数量、类型和内容不同， 这个部分的长度也会有所不同） EOF常量,1字节,标志着rdb文件的结束 check_sum:8字节的校验和,程序通过对REDIS,db_version,databases,EOF四部分计算得来,服务器载入rdb文件时,会将文件计算得来的校验和与该值对比,依次来检测rdb文件正确性 databases为空的结构图如下 databases不为空的结构图如下 TYPE 记录了 value 的类型， 长度为 1 字节， 值可以是以下常量的其中一个，每个 TYPE 常量都代表了一种对象类型或者底层编码， 当服务器读入 RDB 文件中的键值对数据时， 程序会根据 TYPE 的值来决定如何读入和解释 value 的数据123456789REDIS_RDB_TYPE_STRINGREDIS_RDB_TYPE_LISTREDIS_RDB_TYPE_SETREDIS_RDB_TYPE_ZSETREDIS_RDB_TYPE_HASHREDIS_RDB_TYPE_LIST_ZIPLISTREDIS_RDB_TYPE_SET_INTSETREDIS_RDB_TYPE_ZSET_ZIPLISTREDIS_RDB_TYPE_HASH_ZIPLIST key 和 value 分别保存了键值对的键对象和值对象 其中 key 总是一个字符串对象， 它的编码方式和 REDIS_RDB_TYPE_STRING 类型的 value 一样。 根据内容长度的不同， key 的长度也会有所不同 根据 TYPE 类型的不同， 以及保存内容长度的不同， 保存 value 的结构和长度也会有所不同 value编码-字符串对象1、如果 TYPE 的值为 REDIS_RDB_TYPE_STRING ， 那么 value 保存的就是一个字符串对象， 字符串对象的编码可以是 REDIS_ENCODING_INT 或者 REDIS_ENCODING_RAW，如果字符串对象的编码为 REDIS_ENCODING_INT ， 那么说明对象中保存的是长度不超过 32 位的整数，结构如下| ENCODING | integer || ————- |:————-:| ENCODING 的值可以是 REDIS_RDB_ENC_INT8 、 REDIS_RDB_ENC_INT16 或者 REDIS_RDB_ENC_INT32 三个常量的其中一个， 它们分别代表 RDB 文件使用 8 位（bit）、 16 位或者 32 位来保存整数值 integer，如果字符串对象中保存的是可以用 8 位来保存的整数 123 ， 结构如下 | REDIS_RDB_ENC_INT8 | 123 || ————- |:————-:| 如果字符串对象的编码为 REDIS_ENCODING_RAW ， 那么说明对象所保存的是一个字符串值， 根据字符串长度的不同， 有压缩和不压缩两种方法来保存这个字符串 如果字符串的长度小于等于 20 字节， 那么这个字符串会直接被原样保存。 如果字符串的长度大于 20 字节， 那么这个字符串会被压缩之后再保存。对于没有被压缩的字符串结构如下| len | string || ————- |:————-:| 5 “hello” 对于压缩后的字符串结构如下| REDIS_RDB_ENC_LZF | compressed_len | origin_len | compressed_string || ————- |:————-:| REDIS_RDB_ENC_LZF 6 21 “?aa???” REDIS_RDB_ENC_LZF 常量标志着字符串已经被 LZF 算法（http://liblzf.plan9.de）压缩过了 compressed_len 记录的是字符串被压缩之后的长度 origin_len 记录的是字符串原来的长度 compressed_string 记录的则是被压缩之后的字符串 value编码-列表对象如果 TYPE 的值为 REDIS_RDB_TYPE_LIST ， 那么 value 保存的就是一个 REDIS_ENCODING_LINKEDLIST 编码的列表对象，结构如下| list_length | item1 | item2 | item3 | … | itemN || ————- |:————-:| 3 5 “hello” 5 “world” 1 “!” value编码-集合对象如果 TYPE 的值为 REDIS_RDB_TYPE_SET ， 那么 value 保存的就是一个 REDIS_ENCODING_HT 编码的集合对象，结构如下| set_size | elem1 | elem2 | elem3 | … | elemN || ————- |:————-:| 4 5 “apple” 6 “banana” 3 “cat” 3 “pig” value编码-哈希表对象如果 TYPE 的值为 REDIS_RDB_TYPE_HASH ， 那么 value 保存的就是一个 REDIS_ENCODING_HT 编码的集合对象，结构如下| hash_size | key_value_pair1 | key_value_pair2 | key_value_pair3 | … | key_value_pairN || ————- |:————-:| 2 1 “a” 6 “banana” 1 “b” 3 “pig” value编码-有序集合对象如果 TYPE 的值为 REDIS_RDB_TYPE_ZSET ， 那么 value 保存的就是一个 REDIS_ENCODING_SKIPLIST 编码的有序集合对象，结构如下| sorted_set_size | member1 | score1 | member2 | score2 | … | memberN | scoreN || ————- |:————-:| 2 2 “pi” 4 “3.14” 1 “e” 3 “2.7” 第一个元素的成员是长度为 2 的字符串 “pi” ， 分值被转换成字符串之后变成了长度为 4 的字符串 “3.14” 第二个元素的成员是长度为 1 的字符串 “e” ， 分值被转换成字符串之后变成了长度为 3 的字符串 “2.7” value编码-INTSET 编码的集合如果 TYPE 的值为 REDIS_RDB_TYPE_SET_INTSET ， 那么 value 保存的就是一个整数集合对象， RDB 文件保存这种对象的方法是， 先将整数集合转换为字符串对象， 然后将这个字符串对象保存到 RDB 文件里面。 如果程序在读入 RDB 文件的过程中， 碰到由整数集合对象转换成的字符串对象， 那么程序会根据 TYPE 值的指示， 先读入字符串对象， 再将这个字符串对象转换成原来的整数集合对象。 ZIPLIST 编码的列表、哈希表或者有序集合如果 TYPE 的值为 REDIS_RDB_TYPE_LIST_ZIPLIST 、 REDIS_RDB_TYPE_HASH_ZIPLIST 或者 REDIS_RDB_TYPE_ZSET_ZIPLIST ， 那么 value 保存的就是一个压缩列表对象， RDB 文件保存这种对象的方法是： 将压缩列表转换成一个字符串对象。 将转换所得的字符串对象保存到 RDB 文件。 如果程序在读入 RDB 文件的过程中， 碰到由压缩列表对象转换成的字符串对象， 那么程序会根据 TYPE 值的指示， 执行以下操作： 读入字符串对象，并将它转换成原来的压缩列表对象。 根据 TYPE 的值，设置压缩列表对象的类型： 如果 TYPE 的值为 REDIS_RDB_TYPE_LIST_ZIPLIST ， 那么压缩列表对象的类型为列表； 如果 TYPE 的值为 REDIS_RDB_TYPE_HASH_ZIPLIST ， 那么压缩列表对象的类型为哈希表； 如果 TYPE 的值为 REDIS_RDB_TYPE_ZSET_ZIPLIST ， 那么压缩列表对象的类型为有序集合。 从步骤 2 可以看出， 由于 TYPE 的存在， 即使列表、哈希表和有序集合三种类型都使用压缩列表来保存， RDB 读入程序也总可以将读入并转换之后得出的压缩列表设置成原来的类型。 AOF持久化当 AOF 持久化功能处于打开状态时， 服务器在执行完一个写命令之后， 会以协议格式将被执行的写命令追加到服务器状态的 aof_buf 缓冲区的末尾，AOF持久化的功能实现可描述为命令追加(append)，文件写入，文件同步(sync)三个步骤 aof-命令追加1234567891011121314//执行命令redis&gt; SET KEY VALUEOK//如下协议内容追加到aof_buf缓冲区中*3\\r\\n$3\\r\\nSET\\r\\n$3\\r\\nKEY\\r\\n$5\\r\\nVALUE\\r\\n//执行命令redis&gt; RPUSH NUMBERS ONE TWO THREE(integer) 3//如下协议内容追加到aof_buf缓冲区中*5\\r\\n$5\\r\\nRPUSH\\r\\n$7\\r\\nNUMBERS\\r\\n$3\\r\\nONE\\r\\n$3\\r\\nTWO\\r\\n$5\\r\\nTHREE\\r\\n aof-文件写入与同步Redis 的服务器进程就是一个事件循环（loop）， 这个循环中的文件事件负责接收客户端的命令请求， 以及向客户端发送命令回复， 而时间事件则负责执行像 serverCron 函数这样需要定时运行的函数。 因为服务器在处理文件事件时可能会执行写命令， 使得一些内容被追加到 aof_buf 缓冲区里面， 所以在服务器每次结束一个事件循环之前， 它都会调用 flushAppendOnlyFile 函数， 考虑是否需要将 aof_buf 缓冲区中的内容写入和保存到 AOF 文件里面， 这个过程可以用以下伪代码表示：1234567891011def eventLoop(): while True: # 处理文件事件，接收命令请求以及发送命令回复 # 处理命令请求时可能会有新内容被追加到 aof_buf 缓冲区中 processFileEvents() # 处理时间事件 processTimeEvents() # 考虑是否要将 aof_buf 中的内容写入和保存到 AOF 文件里面 flushAppendOnlyFile()# flushAppendOnlyFile 函数的行为由服务器配置的 appendfsync 选项的值来决定（always、everysec、no） 文件的写入和同步原理如下 为了提高文件的写入效率， 在现代操作系统中， 当用户调用 write 函数， 将一些数据写入到文件的时候， 操作系统通常会将写入数据暂时保存在一个内存缓冲区里面， 等到缓冲区的空间被填满、或者超过了指定的时限之后， 才真正地将缓冲区中的数据写入到磁盘里面。 这种做法虽然提高了效率， 但也为写入数据带来了安全问题， 因为如果计算机发生停机， 那么保存在内存缓冲区里面的写入数据将会丢失。 为此， 系统提供了 fsync 和 fdatasync 两个同步函数， 它们可以强制让操作系统立即将缓冲区中的数据写入到硬盘里面， 从而确保写入数据的安全性。 AOF重写原理如下 由于AOF文件更行频率很高,用户会有大量的写命令,如果每次都记录,则会浪费大量空间,所以Redis实现了AOF重写功能:首先从数据库中读取键现在的值,然后用一条命令去记录键值对,代替之前记录这个键值对的多条命令 由于redis会伴随大量的写入操作,如果服务器去执行aof重写,则可能长时间阻塞,于是服务器使用子进程来进行aof重写,子进程持有服务器进程的数据副本.然而在子进程每次重写期间,服务器又会有新的写请求,那么如何解决这个数据不一致问题呢?为了解决这个问题,Redis服务器设置了一个aof重写缓冲区,在创建了子进程时,开始使用缓冲区,在子进程重写期间,每当Redis服务器有新的写操作,都会把命令同时发给aof缓冲区和重写缓冲区,这样一来 AOF缓冲区的内容会定期被写人和同步到AOF文件，对现有AOF文件的处理工作如常进行 从创建子进程开始，服务器执行的所有写命令都会被记录到AOF重写缓冲区里面. 当子进程完成AOF重写工作之后，它会向父进程发送一个信号,父进程在接到该信号后: 将AOF重写缓冲区中的所有内容写入到新AOF文件中，这时新AOF文件所保存的数据库状态将和服务器当前的数据库状态一致 对新的AOF文件进行改名，原子地(atomic)覆盖现有的AOF文件，完成新旧两个AOF文件的替换 AOF 持久化的效率和安全性 服务器配置 appendfsync 选项的值直接决定 AOF 持久化功能的效率和安全性。 当 appendfsync 的值为 always 时， 服务器在每个事件循环都要将 aof_buf 缓冲区中的所有内容写入到 AOF 文件， 并且同步 AOF 文件， 所以 always 的效率是 appendfsync 选项三个值当中最慢的一个， 但从安全性来说， always 也是最安全的， 因为即使出现故障停机， AOF 持久化也只会丢失一个事件循环中所产生的命令数据。 当 appendfsync 的值为 everysec 时， 服务器在每个事件循环都要将 aof_buf 缓冲区中的所有内容写入到 AOF 文件， 并且每隔超过一秒就要在子线程中对 AOF 文件进行一次同步： 从效率上来讲， everysec 模式足够快， 并且就算出现故障停机， 数据库也只丢失一秒钟的命令数据。 当 appendfsync 的值为 no 时， 服务器在每个事件循环都要将 aof_buf 缓冲区中的所有内容写入到 AOF 文件， 至于何时对 AOF 文件进行同步， 则由操作系统控制。 因为处于 no 模式下的 flushAppendOnlyFile 调用无须执行同步操作， 所以该模式下的 AOF 文件写入速度总是最快的， 不过因为这种模式会在系统缓存中积累一段时间的写入数据， 所以该模式的单次同步时长通常是三种模式中时间最长的： 从平摊操作的角度来看， no 模式和 everysec 模式的效率类似， 当出现故障停机时， 使用 no 模式的服务器将丢失上次同步 AOF 文件之后的所有写命令数据。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://grooes.com/tags/redis/"}]},{"title":"redis之数据库篇","slug":"redis之数据库篇","date":"2017-09-28T12:00:35.000Z","updated":"2017-09-28T12:01:19.044Z","comments":true,"path":"2017/09/28/redis之数据库篇/","link":"","permalink":"http://grooes.com/2017/09/28/redis之数据库篇/","excerpt":"","text":"redis设计与实现之数据库篇Redis 是一个键值对（key-value pair）数据库服务器，所有数据库都保存在服务器状态redis.h/redisServer结构的db中,db是一个redisDb数组类型,每个元素都代表一个数据库(redisDb),而服务器内部又保存一个redisClient结构,它包含了一个redisDb指针,用来表示当前所用数据库.当我们想切换数据库时,只需修改这个指针即可， 服务器中的每个数据库都由一个 redis.h/redisDb 结构表示， 其中， redisDb 结构的 dict 字典保存了数据库中的所有键值对， 我们将这个字典称为键空间（key space），数据结构如下1234567891011121314151617181920struct redisServer&#123; ... redisDb *db; //redisDb数组,表示服务器中所有的数据库 int dbnum; //数组大小,默认为16 ... &#125;; typedef struct redisClient&#123; ... redisDb *db; //客户端当前所选数据库 ... &#125;redisClient;typedef struct redisDb &#123; int id; // 数据库ID标识 dict *dict; // 数据库键空间，存放着所有的键值对 dict *expires; // 键的过期时间 dict *watched_keys; // 被watch命令监控的key和相应client long long avg_ttl; // 数据库内所有键的平均TTL（生存时间） &#125; redisDb; 代码中有个dict,这个字典(键空间)存放了数据库所有的键值对,它的键是字符串对象,值可以使Redis的5中任意对象之一.下面给出一个它的实例图，执行命令如下1234567891011121314redis&gt; SET message &quot;hello world&quot;OKredis&gt; RPUSH alphabet &quot;a&quot; &quot;b&quot; &quot;c&quot;(integer) 3redis&gt; HSET book name &quot;Redis in Action&quot;(integer) 1redis&gt; HSET book author &quot;Josiah L. Carlson&quot;(integer) 1redis&gt; HSET book publisher &quot;Manning&quot;(integer) 1 图结构如下 再执行命令如下12redis&gt; SET date &quot;2013.12.1&quot;(integer) 1 图结构如下 再执行命令如下12redis&gt; DEL book(integer) 1 图结构如下 再执行命令如下12redis&gt; SET message &quot;blah blah&quot;(integer) 1 图结构如下 再执行命令如下12redis&gt; HSET book page 320(integer) 1 图结构如下 再执行命令如下12redis&gt; GET message&quot;hello world&quot; 图结构如下 再执行命令如下1234redis&gt; LRANGE alphabet 0 -11) &quot;a&quot;2) &quot;b&quot;3) &quot;c&quot; 图结构如下 当使用 Redis 命令对数据库进行读写时， 服务器不仅会对键空间执行指定的读写操作， 还会执行一些额外的维护操作， 其中包括 在读取一个键之后（读操作和写操作都要对键进行读取）， 服务器会根据键是否存在， 以此来更新服务器的键空间命中（hit）次数或键空间不命中（miss）次数， 这两个值可以在 INFO stats 命令的 keyspace_hits 属性和 keyspace_misses 属性中查看 在读取一个键之后， 服务器会更新键的 LRU （最后一次使用）时间， 这个值可以用于计算键的闲置时间， 使用命令 OBJECT idletime 命令可以查看键 key 的闲置时间 如果服务器在读取一个键时， 发现该键已经过期， 那么服务器会先删除这个过期键， 然后才执行余下的其他操作， 本章稍后对过期键的讨论会详细说明这一点 如果有客户端使用 WATCH 命令监视了某个键， 那么服务器在对被监视的键进行修改之后， 会将这个键标记为脏（dirty）， 从而让事务程序注意到这个键已经被修改过 服务器每次修改一个键之后， 都会对脏（dirty）键计数器的值增一， 这个计数器会触发服务器的持久化以及复制操作执行 如果服务器开启了数据库通知功能， 那么在对键进行修改之后， 服务器将按配置发送相应的数据库通知 生存周期 设置键生存时间 EXPIRE key seconds, PEXPIRE key ms 设置过期时间: EXPIREAT unix时间戳(s) , PEXPIREAT unix时间戳(ms) 过期删除策略 定时删除:通过设置定时器,在过期时立即删除 惰性删除:放任不管,但在获取键时发现过期,则删除 定期删除:每隔一段时间,检查一次数据库,删除过期键 定时删除占用CPU,影响服务器响应时间和吞吐量;惰性删除太占用空间,可能发生内存泄露,定期删除是两者的中和,服务器需要根据情况,合理的设置删除操作的执行时长和执行频率. 下面看下代码是如何执行的12345678910111213141516171819202122232425262728293031323334353637# 默认每次检查的数据库数量DEFAULT_DB_NUMBERS = 16# 默认每个数据库检查的键数量DEFAULT_KEY_NUMBERS = 20# 全局变量，记录检查进度current_db = 0def activeExpireCycle(): #初始化要检查的数据库数量 #如果服务榕的数据库数量比DEFAULT DB NUMBERS 要小 #那么以服务器的数据库数量为准 if server.dbnum &lt; DEFAULT_DB_NUMBERS : db_numbers = server.dbnum else : db_numbers = DEFAULT_DB_NUMBERS #遍历各个数据库 for i in range(db_numbers) : #如果current_db 的值等于服务榕的数据库数量 #这表示检查程序已经遍历了服务榕的所有数据库一次 #将current_db重置为0 ，开始新的一轮遍历 if current_db == server.dbnum: current_db = 0 # 获取当前要处理的数据库 redisDB = server.db[current db) #将数据库索引增1 ，指向下一个要处理的数据库 current_db += 1 #检查数据库键 for j in range(DEFAULT_KEY_NUMBERS): #如果数据库中没有一个键带有过期时间，那么跳过这个数据库 if redisDB.expires.size () == 0: break # 随机获取一个带有过期时间的键 key with_ttl = redisDb.expires.get_random_key () #检查键是否过期，如果过期就删除它 if is_expired (key with ttl): delete_key (key_with_ttl ) #已达到时间上限，停止处理 if reach_time_limit(): return activeExpireCycle函数的工作模式可以总结如下 函数每次运行时，都从一定数量的数据库中取出一定数量的随机键进行检查， 并删除其中的过期键 全局变量current db 会记录当前activeExpireCycle 函数检查的进度，并在下一次activeExpireCycle 函数调用时，接着上一次的进度进行处理。比如说，如果当前activeExpireCycle 函数在遍历10 号数据库时返回了，那么下次activeExpireCycle 函数执行时，将从11号数据库开始查找并删除过期键 随着activeExpireCycle函数的不断执行，服务器中的所有数据库都会被检查一遍，这时函数将current_db变量重置为0,然后再次开始新一轮的检查工作","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://grooes.com/tags/redis/"}]},{"title":"redis之对象篇","slug":"redis之对象篇","date":"2017-09-24T14:09:39.000Z","updated":"2017-09-28T12:00:58.873Z","comments":true,"path":"2017/09/24/redis之对象篇/","link":"","permalink":"http://grooes.com/2017/09/24/redis之对象篇/","excerpt":"","text":"redis设计与实现之对象篇Redis并没直接使用这些数据结构去实现key-value pair数据库,而是基于这些数据结构实现了一系列对象.这些对象有:字符串对象,列表对象,集合对象,有序集合对象,哈希对象 五种基本对象，redis是如何表示对象的？1234567typedef struct redisObject &#123; unsigned type:4; // 类型 unsigned encoding:4;// 编码方式 void *ptr; // 指向对象的值 int refcount;// 引用计数,可用来对象回收与共享 unsigned lru:22;//对象最后一次被访问的时间 &#125; robj; type详解对象的 type 属性记录了对象的类型（利用命令：type key即可获取）| 对象 | 对象type属性的值 | TYPE 命令的输出 || :—: | :—-: | :——-: || 字符串对象 | REDIS_STRING | string || 列表对象 | REDIS_LIST | list || 哈希对象 | REDIS_HASH | hash || 集合对象 | REDIS_SET | set || 有序集合对象 | REDIS_ZSET | zset | encoding详解对象的 ptr 指针指向对象的底层实现数据结构， 而这些数据结构由对象的 encoding 属性决定，encoding 属性记录了对象所使用的编码， 也即是说这个对象使用了什么数据结构作为对象的底层实现，每种类型的对象都至少使用了两种不同的编码（利用object encoding key 可获取）| 类型 | 编码 | 对象 | BJECT ENCODING|| :— | :—- | :——-| :——-|| REDIS_STRING | REDIS_ENCODING_INT | 使用整数值实现的字符串对象 | int || REDIS_STRING | REDIS_ENCODING_EMBSTR | 使用 embstr 编码的简单动态字符串实现的字符串对象 | embstr || REDIS_STRING | REDIS_ENCODING_RAW | 使用简单动态字符串实现的字符串对象 | raw || REDIS_LIST | REDIS_ENCODING_ZIPLIST | 使用压缩列表实现的列表对象 | ziplist || REDIS_LIST | REDIS_ENCODING_LINKEDLIST | 使用双端链表实现的列表对象 | linkedlist || REDIS_HASH | REDIS_ENCODING_ZIPLIST | 使用压缩列表实现的哈希对象 | ziplist || REDIS_HASH | REDIS_ENCODING_HT | 使用字典实现的哈希对象 | hashtable || REDIS_SET | REDIS_ENCODING_INTSET | 使用整数集合实现的集合对象 | intset || REDIS_SET | REDIS_ENCODING_HT | 使用字典实现的集合对象 | hashtable| REDIS_ZSET | REDIS_ENCODING_ZIPLIST | 使用压缩列表实现的有序集合对象 | intset || REDIS_ZSET | REDIS_ENCODING_SKIPLIST | 使用跳跃表和字典实现的有序集合对象 | skiplist | 字符串对象字符串对象的编码可以是 int 、 embstr 或者 raw，下面看下存储结构是怎么样的？1234567#define REDIS_ENCODING_EMBSTR_SIZE_LIMIT 39 robj *createStringObject(char *ptr, size_t len) &#123; if (len &lt;= REDIS_ENCODING_EMBSTR_SIZE_LIMIT) return createEmbeddedStringObject(ptr,len); else return createRawStringObject(ptr,len); &#125; set一个整型之后的结构 如果字符串对象保存的是一个字符串值， 并且这个字符串值的长度小于等于 39 字节， 那么字符串对象将使用 embstr 编码的方式来保存这个字符串值，embstr 编码是专门用于保存短字符串的一种优化编码方式， 这种编码和 raw 编码一样， 都使用 redisObject 结构和 sdshdr 结构来表示字符串对象， 但 raw 编码会调用两次内存分配函数来分别创建 redisObject 结构和 sdshdr 结构， 而 embstr 编码则通过调用一次内存分配函数来分配一块连续的空间， 空间中依次包含 redisObject 和 sdshdr 两个结构 如果字符串对象保存的是一个字符串值， 并且这个字符串值的长度大于 39 字节， 那么字符串对象将使用一个简单动态字符串（SDS）来保存这个字符串值， 并将对象的编码设置为 raw 为什么是39字节，而不是别的？ embstr 编码的字符串对象在执行命令时， 产生的效果和 raw 编码的字符串对象执行命令时产生的效果是相同的， 但使用 embstr 编码的字符串对象来保存短字符串值有以下好处 embstr 编码将创建字符串对象所需的内存分配次数从 raw 编码的两次降低为一次 释放 embstr 编码的字符串对象只需要调用一次内存释放函数， 而释放 raw 编码的字符串对象需要调用两次内存释放函数 因为 embstr 编码的字符串对象的所有数据都保存在一块连续的内存里面， 所以这种编码的字符串对象比起 raw 编码的字符串对象能够更好地利用缓存带来的优势 编码的转换 对于 int 编码的字符串对象来说， 如果我们向对象执行了一些命令， 使得这个对象保存的不再是整数值， 而是一个字符串值， 那么字符串对象的编码将从 int 变为 raw Redis 没有为 embstr 编码的字符串对象编写任何相应的修改程序 （只有 int 编码的字符串对象和 raw 编码的字符串对象有这些程序）， 所以 embstr 编码的字符串对象实际上是只读的： 当我们对 embstr 编码的字符串对象执行任何修改命令时， 程序会先将对象的编码从 embstr 转换成 raw ， 然后再执行修改命令； 因为这个原因， embstr 编码的字符串对象在执行修改命令之后， 总会变成一个 raw 编码的字符串对象 列表对象列表对象的编码可以是 ziplist 或者 linkedlistziplist的结构如下（执行rpush numbers 1 “three” 5） linkedlist的结构如下 其中StringObject就是图8.4的结构当列表对象可以同时满足以下两个条件时， 列表对象使用 ziplist 编码 列表对象保存的所有字符串元素的长度都小于 64 字节（可通过配置文件里 list-max-ziplist-value 调节） 列表对象保存的元素数量小于 512 个 （可通过配置文件里 list-max-ziplist-entries 调节） 哈希对象哈希对象的编码可以是 ziplist 或者 hashtableziplist 编码的哈希对象使用压缩列表作为底层实现， 每当有新的键值对要加入到哈希对象时， 程序会先将保存了键的压缩列表节点推入到压缩列表表尾， 然后再将保存了值的压缩列表节点推入到压缩列表表尾，因此 保存了同一键值对的两个节点总是紧挨在一起， 保存键的节点在前， 保存值的节点在后 先添加到哈希对象中的键值对会被放在压缩列表的表头方向， 而后来添加到哈希对象中的键值对会被放在压缩列表的表尾方向 ziplist结构(执行HSET profile name “Tom”；HSET profile age 25；HSET profile career “Programmer”) hashtable结构 当哈希对象可以同时满足以下两个条件时， 哈希对象使用 ziplist 编码 哈希对象保存的所有键值对的键和值的字符串长度都小于 64 字节（可通过配置文件里 hash-max-ziplist-value 调节） 哈希对象保存的键值对数量小于 512 个（可通过配置文件里 hash-max-ziplist-entries 调节） 集合对象集合对象的编码可以是 intset 或者 hashtableinset结构（SADD numbers 1 3 5） hashtable结构（SADD fruits “apple” “banana” “cherry”） 当集合对象可以同时满足以下两个条件时， 对象使用 intset 编码 集合对象保存的所有元素都是整数值 集合对象保存的元素数量不超过 512 个（可通过配置文件里 set-max-intset-entries 调节） 有序集合对象有序集合的编码可以是 ziplist 或者 skiplist1234typedef struct zset &#123; zskiplist *zsl;//跳跃表 dict *dict;//字典&#125; zset; ziplist 编码的有序集合对象使用压缩列表作为底层实现， 每个集合元素使用两个紧挨在一起的压缩列表节点来保存， 第一个节点保存元素的成员（member）， 而第二个元素则保存元素的分值（score），压缩列表内的集合元素按分值从小到大进行排序， 分值较小的元素被放置在靠近表头的方向， 而分值较大的元素则被放置在靠近表尾的方向ziplist结构（执行ZADD price 8.5 apple 5.0 banana 6.0 cherry） skiplist结构 当有序集合对象可以同时满足以下两个条件时， 对象使用 ziplist 编码 有序集合保存的元素数量小于 128 个（可通过配置文件里 zset-max-ziplist-entries 调节） 有序集合保存的所有元素成员的长度都小于 64 字节（可通过配置文件里 zset-max-ziplist-value 调节） 为什么有序集合需要同时使用跳跃表和字典来实现？ 在理论上来说， 有序集合可以单独使用字典或者跳跃表的其中一种数据结构来实现， 但无论单独使用字典还是跳跃表， 在性能上对比起同时使用字典和跳跃表都会有所降低。 举个例子， 如果我们只使用字典来实现有序集合， 那么虽然以 O(1) 复杂度查找成员的分值这一特性会被保留， 但是， 因为字典以无序的方式来保存集合元素， 所以每次在执行范围型操作 —— 比如 ZRANK 、 ZRANGE 等命令时， 程序都需要对字典保存的所有元素进行排序， 完成这种排序需要至少 O(N \\log N) 时间复杂度， 以及额外的 O(N) 内存空间 （因为要创建一个数组来保存排序后的元素）。 另一方面， 如果我们只使用跳跃表来实现有序集合， 那么跳跃表执行范围型操作的所有优点都会被保留， 但因为没有了字典， 所以根据成员查找分值这一操作的复杂度将从 O(1) 上升为 O(\\log N) 。 因为以上原因， 为了让有序集合的查找和范围型操作都尽可能快地执行， Redis 选择了同时使用字典和跳跃表两种数据结构来实现有序集合。 类型检查与命令多态Redis 中用于操作键的命令基本上可以分为两种类型针对任何key都可以执行 DEL 命令 EXPIRE 命令 RENAME 命令 TYPE 命令 OBJECT 命令 特定key才能执行 SET 、 GET 、 APPEND 、 STRLEN 等命令只能对字符串键执行 HDEL 、 HSET 、 HGET 、 HLEN 等命令只能对哈希键执行 RPUSH 、 LPOP 、 LINSERT 、 LLEN 等命令只能对列表键执行 SADD 、 SPOP 、 SINTER 、 SCARD 等命令只能对集合键执行 ZADD 、 ZCARD 、 ZRANK 、 ZSCORE 等命令只能对有序集合键执行 如何针对特定的key才可以检查的流程图如下 内存回收Redis 在自己的对象系统中构建了一个引用计数（reference counting）技术实现的内存回收机制， 通过这一机制， 程序可以通过跟踪对象的引用计数信息， 在适当的时候自动释放对象并进行内存回收（refcount支配），对象的整个生命周期可以划分为创建对象、操作对象、释放对象三个阶段 在创建一个新对象时， 引用计数的值会被初始化为 1 当对象被一个新程序使用时， 它的引用计数值会被增一（api -&gt; incrRefCount） 当对象不再被一个程序使用时， 它的引用计数值会被减一（api -&gt; decrRefCount） 当对象的引用计数值变为 0 时， 对象所占用的内存会被释放（api -&gt; resetRefCount） 对象共享 创建共享字符串对象的数量可以通过修改 redis.h/REDIS_SHARED_INTEGERS 常量来修改OBJECT REFCOUNT key 命令来查看共享数量 让多个键共享同一个值对象需要执行以下两个步骤 将数据库键的值指针指向一个现有的值对象 将被共享的值对象的引用计数增一 Redis 会在初始化服务器时， 创建一万个字符串对象， 这些对象包含了从 0 到 9999 的所有整数值， 当服务器需要用到值为 0 到 9999 的字符串对象时， 服务器就会使用这些共享对象， 而不是新创建对象 为什么 Redis 不共享包含字符串的对象？ 当服务器考虑将一个共享对象设置为键的值对象时， 程序需要先检查给定的共享对象和键想创建的目标对象是否完全相同， 只有在共享对象和目标对象完全相同的情况下， 程序才会将共享对象用作键的值对象， 而一个共享对象保存的值越复杂， 验证共享对象和目标对象是否相同所需的复杂度就会越高， 消耗的 CPU 时间也会越多： 如果共享对象是保存整数值的字符串对象， 那么验证操作的复杂度为 O(1) 如果共享对象是保存字符串值的字符串对象， 那么验证操作的复杂度为 O(N) 如果共享对象是包含了多个值（或者对象的）对象， 比如列表对象或者哈希对象， 那么验证操作的复杂度将会是 O(N^2) 因此， 尽管共享更复杂的对象可以节约更多的内存， 但受到 CPU 时间的限制， Redis 只对包含整数值的字符串对象进行共享 对象的空转时长OBJECT IDLETIME 命令可以打印出给定键的空转时长， 这一空转时长就是通过将当前时间减去键的值对象的 lru 时间计算得出的 OBJECT IDLETIME 命令的实现是特殊的， 这个命令在访问键的值对象时， 不会修改值对象的 lru 属性 除了可以被 OBJECT IDLETIME 命令打印出来之外， 键的空转时长还有另外一项作用： 如果服务器打开了 maxmemory 选项， 并且服务器用于回收内存的算法为 volatile-lru 或者 allkeys-lru ， 那么当服务器占用的内存数超过了 maxmemory 选项所设置的上限值时， 空转时长较高的那部分键会优先被服务器释放， 从而回收内存","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://grooes.com/tags/redis/"}]},{"title":"redis之二进制位数组篇","slug":"redis之二进制位数组篇","date":"2017-09-24T01:41:27.000Z","updated":"2017-09-24T02:29:47.368Z","comments":true,"path":"2017/09/24/redis之二进制位数组篇/","link":"","permalink":"http://grooes.com/2017/09/24/redis之二进制位数组篇/","excerpt":"","text":"redis设计与实现之二进制位数组篇redis提供了setbit、getbit、bitcount、bitop四个命令用于处理二进制位数组 setbit命令用于为位数组指定偏移量上的二进制设置值，只能赋值0或者1getbit命令用于获取位数组指定偏移量上的二进制位的值bitcount命令用于统计位数组里面，值为1的二进制位的数量bitop命令既可以对多个数组进行按位与（and）按位或（or）按位异或（xor） 位数组的表示 注释：buf数组采用逆序来保存位数组 位数组命令的实现getbit命令实现1234567891011121314GETBIT &lt;bitarray&gt; &lt;offset&gt;计算 byte = [offset / 8] ， byte 值记录了 offset 偏移量指定的二进制位保存在位数组的哪个字节计算 bit = (offset mod 8) + 1 ， bit 值记录了 offset 偏移量指定的二进制位是 byte 字节的第几个二进制位根据 byte 值和 bit 值， 在位数组 bitarray 中定位 offset 偏移量指定的二进制位， 并返回这个位的值(返回的是原来的值，切记)case如下GETBIT &lt;bitarray&gt; 3[3 / 8] 的值为 0(3 mod 8) + 1 的值为 4 定位到 buf[0] 字节上面， 然后取出该字节上的第 4 个二进制位（从左向右数）的值向客户端返回二进制位的值 (如果是1返回1，如果是0返回0)注释：时间复杂度位O(1) setbit命令实现12345678910SETBIT &lt;bitarray&gt; &lt;offset&gt; &lt;value&gt;计算len=[offset / 8]+1，len值记录了保存offset偏移量指定的二进制位至少需要多少字节检查bitarray键保存的位数组（sds）的长度是否小于len，如果是则将sds的长度扩展为len字节，并将所有新扩展空间的二进制位的值设为0计算byte=[offset / 8]，byte值记录了offset偏移量指定的二进制位保存在位数组的哪个字节计算bit=(offset mod 8)+1，bit值记录了offset偏移量指定的二进制位是byte字节的第几个二进制位根据byte值和bit值，在bitarray键保存的位数组中定位offset偏移量指定的二进制位，首先将制定二进制位现在值保存在oldvalue变量，然后将新值value设置为这个二进制位的值向客户端返回oldvalue变量的值注释：时间复杂度位O(1) bitcount命令实现bitcount命令的实现用到了查表和variable-precisionSWAR两种算法12345678查表算法使用键长位8位的表，表中记录了从0000 0000到1111 1111 在内的所有二进制位的汉明重量至于variable-precisionSWAR算法方面，bitcount命令在每次循环中载入128个二进制位，然后调用四次32位variable-precisionSWAR算法来计算这128个二进制位的汉明重量在执行bitcount命令时，程序会根据未处理的二进制位的数量来决定使用哪种算法如果未出的二进制位的数量大于等于128位，那么程序使用variable-precisionSWAR算法计算汉明重量如果未出的二进制位的数量小于128位，那么程序使用查表法计算汉明重量注释：时间复杂度位O(n) bitop命令实现12没什么好说的，和C语言一样的逻辑操作因为bitop and、bitop or、bitop xor三个命令可以接受多个位数作为输入，程序需要遍历输入的每个位数组的每个字节来进行计算，所有这些命令的复杂度位O(n^2)，与此相反，bitop not命令只接受一个位数组的输入，复杂度位O(n)","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://grooes.com/tags/redis/"}]},{"title":"redis之数据结构篇","slug":"redis之数据结构篇","date":"2017-09-23T08:33:59.000Z","updated":"2017-09-23T13:27:12.855Z","comments":true,"path":"2017/09/23/redis之数据结构篇/","link":"","permalink":"http://grooes.com/2017/09/23/redis之数据结构篇/","excerpt":"","text":"redis设计与实现之数据结构篇简单动态字符串(sds) 从数据结构看sds为buf分配了5个字符和0个free空间，\\0是不计算在len内，但是会分配额外的1字节空间，下面讲讲这样设计的好处 数据结构中存了len字段，可以O(1)常数时间获取字符串长度 每次插入元素前都会检查sds空间是否足够，若不满足做适当的扩充，可以避免缓冲区溢出的可能性(C语言中每次插入或者删除字符的时候都要重新分配新的内存来保存新值，是一个很耗时的操作 ) sds采用空间预分配和惰性空间释放两种策略减少了内存重分配的次数，看到free字段就知道这是一个额外的空间来在某种程度上减少了重分配的次数，具体如下 123如现有空间足够存放修改后内容，则不进行扩展，直接插入。如现有空间不足以存放修改后内容，且修改后sds的len小于1M，则扩展后len=free，比如修改后空间为15 byte，则分配的空间是31：15+15+1byte.如现有空间不足以存放修改后内容，且修改后sds的len大于1M，则分配1M的使用空间，则新的空间为20M+1M+1byte 惰性空间释放:用于优化SDS缩减操作.当api对sds进行缩短操作后,程序不会释放缩短后空余出的空间,而是使用free属性把他们记录下来,当我们插入的时候就可以减少内存重分配了(不需要担心，我们以后不添加内容时，造成的内存泄露，因为sds提供了释放空间的API) 二进制安全，不像C语言，C字符串的字符必须某种编码(比如ASCII),并且除末尾外,其余地方不能出现空字符串\\0,否则会被提前结束,(\\0是c字符串的结束标识符),比如acvf\\0sdsdsd\\0,只能识别到acvf，而sds是根据len属性来判断字符串的结束，所以不存在是否有\\0 由于SDS的buf数组以\\0结尾,符合c字符串特征,因此,它可以使用C字符串的一些api 链表 1234567891011121314struct listNode&#123; struct listNode *prev//前置节点 struct listNode *next//后置节点 void *value //节点值&#125;listNode;struct list&#123; listNode *head;//表头节点 listNode *tail;//表尾节点 unsigned long len;//常数时间获取链表长度 void *(*dup)(void *ptr);//节点值复制函数 void *(*free)(void *ptr);//节点值释放函数 void *(*match)(void *ptr,void *key);//节点值对比函数&#125;list; 链表的实现特性总结如下 双端：每个节点都有prev和next，获取某个节点的前置节点和后置节点的复杂度都是O(1) 无环： 表头节点的 prev 指针和表尾节点的 next 指针都指向 NULL ， 对链表的访问以 NULL 为终点 带表头指针和表尾指针： 通过 list 结构的 head 指针和 tail 指针， 程序获取链表的表头节点和表尾节点的复杂度为 O(1) 带链表长度计数器： 程序使用 list 结构的 len 属性来对 list 持有的链表节点进行计数， 程序获取链表中节点数量的复杂度为 O(1) 多态： 链表节点使用 void* 指针来保存节点值， 并且可以通过 list 结构的 dup 、 free 、 match 三个属性为节点值设置类型特定函数， 所以链表可以用于保存各种不同类型的值 字典字典又称为符号表、关联数组、映射，是一种用于保存键值对的抽象数据结构，下面看某结构实现 哈希表实现12345678910111213141516typedef struct dictht &#123; dictEntry **table;//哈希表数组 unsigned long size;//哈希表大小 unsigned long sizemask;//哈希表大小掩码，用于计算索引值，总是等于 size - 1 unsigned long used;//该哈希表已有节点的数量&#125; dictht;typedef struct dictEntry &#123; void *key;//键 union &#123; void *val; uint64_t u64; int64_t s64; &#125; v;//值 struct dictEntry *next;//指向下个哈希表节点，形成链表&#125; dictEntry; 字典123456789101112131415typedef struct dict &#123; dictType *type;// 类型特定函数，指向 dictType 结构的指针，每个dictType结构保存了一簇用于操作特定类型键值对的函数，Redis 会为用途不同的字典设置不同的类型特定函数。 void *privdata;//私有数据，保存了需要传给那些类型特定函数的可选参数 dictht ht[2];//哈希表，是一个包含两个项的数组，数组中的每个项都是一个dictht哈希表， 一般情况下， 字典只使用ht[0]哈希表， ht[1]哈希表只会在对ht[0]哈希表进行rehash时使用 int rehashidx; //rehash索引，当rehash不在进行时，值为-1&#125; dict;typedef struct dictType &#123; unsigned int (*hashFunction)(const void *key);//计算哈希值的函数 void *(*keyDup)(void *privdata, const void *key);//复制键的函数 void *(*valDup)(void *privdata, const void *obj);//复制值的函数 int (*keyCompare)(void *privdata, const void *key1, const void *key2);//对比键的函数 void (*keyDestructor)(void *privdata, void *key);//销毁键的函数 void (*valDestructor)(void *privdata, void *obj);//销毁值的函数&#125; dictType; 下图展示一个没有进行rehash的字典 哈希算法当要将一个新的键值对添加到字典里面时， 程序需要先根据键值对的键计算出哈希值和索引值， 然后再根据索引值， 将包含新键值对的哈希表节点放到哈希表数组的指定索引上面，Redis 计算哈希值和索引值的方法如下12345//使用字典设置的哈希函数，计算键 key 的哈希值hash = dict-&gt;type-&gt;hashFunction(key);//使用哈希表的 sizemask 属性和哈希值，计算出索引值，根据情况不同，ht[x] 可以是 ht[0] 或者 ht[1]index = hash &amp; dict-&gt;ht[x].sizemask; 下面是一个空字典的结构图 如上图，如果我们要将一个键值对 k0 和 v0 添加到字典里面， 那么程序会先使用语句12hash = dict-&gt;type-&gt;hashFunction(k0);//计算键 k0 的哈希值，假设计算得出的哈希值为8index = hash &amp; dict-&gt;ht[0].sizemask = 8 &amp; 3 = 0;//计算出键k0的索引值0，这表示包含键值对k0和v0的节点应该被放置到哈希表数组的索引0位置上 解决键冲突当有两个或以上数量的键被分配到了哈希表数组的同一个索引上面时， 我们称这些键发生了冲突（collision），Redis 的哈希表使用链地址法（separate chaining）来解决键冲突： 每个哈希表节点都有一个 next 指针， 多个哈希表节点可以用 next 指针构成一个单向链表， 被分配到同一个索引上的多个节点可以用这个单向链表连接起来， 这就解决了键冲突的问题，如图 k1和k2键冲突的话，直接用单向链表连接起来，并且最新添加的放在表头（原因：dictEntry 节点组成的链表没有指向链表表尾的指针， 所以为了速度考虑， 程序总是将新节点添加到链表的表头位置（复杂度为 O(1)）， 排在其他已有节点的前面） rehash随着操作的不断执行， 哈希表保存的键值对会逐渐地增多或者减少， 为了让哈希表的负载因子（load factor）维持在一个合理的范围之内， 当哈希表保存的键值对数量太多或者太少时， 程序需要对哈希表的大小进行相应的扩展或者收缩， Redis 对字典的哈希表执行 rehash 的步骤如下 为字典的 ht[1] 哈希表分配空间， 这个哈希表的空间大小取决于要执行的操作， 以及 ht[0] 当前包含的键值对数量 （也即是 ht[0].used 属性的值） 12如果执行的是扩展操作， 那么 ht[1] 的大小为第一个大于等于 ht[0].used * 2 的 2^n （2 的 n 次方幂）如果执行的是收缩操作， 那么 ht[1] 的大小为第一个大于等于 ht[0].used 的 2^n 将保存在 ht[0] 中的所有键值对 rehash 到 ht[1] 上面： rehash 指的是重新计算键的哈希值和索引值， 然后将键值对放置到 ht[1] 哈希表的指定位置上 当 ht[0] 包含的所有键值对都迁移到了 ht[1] 之后 （ht[0] 变为空表）， 释放 ht[0] ， 将 ht[1] 设置为 ht[0] ， 并在 ht[1] 新创建一个空白哈希表， 为下一次 rehash 做准备 哈希表的扩展与收缩，当以下条件中的任意一个被满足时， 程序会自动开始对哈希表执行扩展操作1234服务器目前没有在执行 BGSAVE 命令或者 BGREWRITEAOF 命令， 并且哈希表的负载因子大于等于 1服务器目前正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令， 并且哈希表的负载因子大于等于 5当哈希表的负载因子小于 0.1 时， 程序自动开始对哈希表执行收缩操作load_factor = ht[0].used / ht[0].size;//负载因子 = 哈希表已保存节点数量 / 哈希表大小 根据 BGSAVE 命令或 BGREWRITEAOF 命令是否正在执行， 服务器执行扩展操作所需的负载因子并不相同， 这是因为在执行 BGSAVE 命令或 BGREWRITEAOF 命令的过程中， Redis 需要创建当前服务器进程的子进程， 而大多数操作系统都采用写时复制（copy-on-write）技术来优化子进程的使用效率， 所以在子进程存在期间， 服务器会提高执行扩展操作所需的负载因子， 从而尽可能地避免在子进程存在期间进行哈希表扩展操作， 这可以避免不必要的内存写入操作， 最大限度地节约内存 渐进式rehashrehash 动作并不是一次性、集中式地完成的， 而是分多次、渐进式地完成的，原因在于， 如果 ht[0] 里只保存着四个键值对， 那么服务器可以在瞬间就将这些键值对全部 rehash 到 ht[1] ； 但是， 如果哈希表里保存的键值对数量不是四个， 而是四百万、四千万甚至四亿个键值对， 那么要一次性将这些键值对全部 rehash 到 ht[1] 的话， 庞大的计算量可能会导致服务器在一段时间内停止服务，所以哈希表渐进式 rehash 的详细步骤1234为 ht[1] 分配空间， 让字典同时持有 ht[0] 和 ht[1] 两个哈希表在字典中维持一个索引计数器变量 rehashidx ， 并将它的值设置为 0 ， 表示 rehash 工作正式开始在 rehash 进行期间， 每次对字典执行添加、删除、查找或者更新操作时， 程序除了执行指定的操作以外， 还会顺带将 ht[0] 哈希表在 rehashidx 索引上的所有键值对 rehash 到 ht[1] ， 当 rehash 工作完成之后， 程序将 rehashidx 属性的值增一随着字典操作的不断执行， 最终在某个时间点上， ht[0] 的所有键值对都会被 rehash 至 ht[1] ， 这时程序将 rehashidx 属性的值设为 -1 ， 表示 rehash 操作已完成 渐进式 rehash 的好处在于它采取分而治之的方式， 将 rehash 键值对所需的计算工作均滩到对字典的每个添加、删除、查找和更新操作上， 从而避免了集中式 rehash 而带来的庞大计算量因为在进行渐进式 rehash 的过程中， 字典会同时使用 ht[0] 和 ht[1] 两个哈希表， 所以在渐进式 rehash 进行期间， 字典的删除（delete）、查找（find）、更新（update）等操作会在两个哈希表上进行： 比如说， 要在字典里面查找一个键的话， 程序会先在 ht[0] 里面进行查找， 如果没找到的话， 就会继续到 ht[1] 里面进行查找，另外， 在渐进式 rehash 执行期间， 新添加到字典的键值对一律会被保存到 ht[1] 里面， 而 ht[0] 则不再进行任何添加操作： 这一措施保证了 ht[0] 包含的键值对数量会只减不增， 并随着 rehash 操作的执行而最终变成空表 跳跃表跳跃表是一种有序数据结构，通过在每个节点中维持多个指向其他节点的指针，从而达到快速访问节点的目的，支持O(logN)，最坏O(N)复杂度的节点查找，还可以通过顺序性操作来批量处理节点，另外可以和平衡树相媲美，且实现起来更简单，redis中只有 实现有序集合键、集群节点用作内部数据结构 来个地方，再没有其他用途 跳跃表的实现 位于图最左为zskiplist结构 header ：指向跳跃表的表头节点 tail ：指向跳跃表的表尾节点 level ：记录目前跳跃表内，层数最大的那个节点的层数（表头节点的层数不计算在内） length ：记录跳跃表的长度，也即是，跳跃表目前包含节点的数量（表头节点不计算在内） 位于 zskiplist 结构右方的是四个 zskiplistNode 结构， 该结构包含以下属性 层（level）：节点中用 L1 、 L2 、 L3 等字样标记节点的各个层， L1 代表第一层， L2 代表第二层，以此类推。每个层都带有两个属性：前进指针和跨度。前进指针用于访问位于表尾方向的其他节点，而跨度则记录了前进指针所指向节点和当前节点的距离。在上面的图片中，连线上带有数字的箭头就代表前进指针，而那个数字就是跨度。当程序从表头向表尾进行遍历时，访问会沿着层的前进指针进行 后退（backward）指针：节点中用 BW 字样标记节点的后退指针，它指向位于当前节点的前一个节点。后退指针在程序从表尾向表头遍历时使用 分值（score）：各个节点中的 1.0 、 2.0 和 3.0 是节点所保存的分值。在跳跃表中，节点按各自所保存的分值从小到大排列 成员对象（obj）：各个节点中的 o1 、 o2 和 o3 是节点所保存的成员对象 跳跃表节点1234567891011121314151617typedef struct zskiplistNode &#123; struct zskiplistNode *backward;//后退指针 double score;//分值 robj *obj;//成员对象 struct zskiplistLevel &#123; struct zskiplistNode *forward;//前进指针 unsigned int span;//跨度 &#125; level[];//层&#125; zskiplistNode;typedef struct zskiplist &#123; struct zskiplistNode *header, *tail;//表头节点和表尾节点，header 和 tail 指针分别指向跳跃表的表头和表尾节点， 通过这两个指针， 程序定位表头节点和表尾节点的复杂度为 O(1) unsigned long length;//表中节点的数量，通过使用 length 属性来记录节点的数量， 程序可以在 O(1) 复杂度内返回跳跃表的长度 int level;//表中层数最大的节点的层数，level 属性则用于在 O(1) 复杂度内获取跳跃表中层高最大的那个节点的层数量， 注意表头节点的层高并不计算在内&#125; zskiplist; 跳跃表节点的 level 数组可以包含多个元素， 每个元素都包含一个指向其他节点的指针， 程序可以通过这些层来加快访问其他节点的速度， 一般来说， 层的数量越多， 访问其他节点的速度就越快每次创建一个新跳跃表节点的时候， 程序都根据幂次定律 （power law，越大的数出现的概率越小） 随机生成一个介于 1 和 32 之间的值作为 level 数组的大小， 这个大小就是层的“高度”每个层都有一个指向表尾方向的前进指针（level[i].forward 属性）， 用于从表头向表尾方向访问节点层的跨度（level[i].span 属性）用于记录两个节点之间的距离 两个节点之间的跨度越大， 它们相距得就越远 指向 NULL 的所有前进指针的跨度都为 0 ， 因为它们没有连向任何节点 节点的后退指针（backward 属性）用于从表尾向表头方向访问节点： 跟可以一次跳过多个节点的前进指针不同， 因为每个节点只有一个后退指针， 所以每次只能后退至前一个节点节点的分值（score 属性）是一个 double 类型的浮点数， 跳跃表中的所有节点都按分值从小到大来排序节点的成员对象（obj 属性）是一个指针， 它指向一个字符串对象， 而字符串对象则保存着一个 SDS 值为啥 redis 使用跳表(skiplist)而不是使用 red-black？ 整数集合整数集合的实现整数集合(intset)是集合键的底层实现之一，当一个集合只包含整数值元素，并且这个集合的元素数量不多时，redis就会使用此结构实现，用于保存整数值的集合抽象数据结构， 它可以保存类型为 int16_t 、 int32_t 或者 int64_t 的整数值， 并且保证集合中不会出现重复元素12345typedef struct intset &#123; uint32_t encoding;//编码方式 uint32_t length;//集合包含的元素数量 int8_t contents[];//保存元素的数组，整数集合的底层实现，整数集合的每个元素都是 contents 数组的一个数组项（item）， 各个项在数组中按值的大小从小到大有序地排列， 并且数组中不包含任何重复项&#125; intset; 对于encoding，有如下解释 如果 encoding 属性的值为 INTSET_ENC_INT16 ， 那么 contents 就是一个 int16_t 类型的数组， 数组里的每个项都是一个 int16_t 类型的整数值 （最小值为 -32,768 ，最大值为 32,767 ） 如果 encoding 属性的值为 INTSET_ENC_INT32 ， 那么 contents 就是一个 int32_t 类型的数组， 数组里的每个项都是一个 int32_t 类型的整数值 （最小值为 -2,147,483,648 ，最大值为 2,147,483,647 ） 如果 encoding 属性的值为 INTSET_ENC_INT64 ， 那么 contents 就是一个 int64_t 类型的数组， 数组里的每个项都是一个 int64_t 类型的整数值 （最小值为 -9,223,372,036,854,775,808 ，最大值为 9,223,372,036,854,775,807 ） 如上图，展示了另一个整数集合示例 encoding 属性的值为 INTSET_ENC_INT16 ， 表示整数集合的底层实现为 int16_t 类型的数组， 而集合保存的都是 int16_t 类型的整数值 length 属性的值为 5 ， 表示整数集合包含五个元素 contents 数组按从小到大的顺序保存着集合中的五个元素 因为每个集合元素都是 int16_t 类型的整数值， 所以 contents 数组的大小等于 sizeof(int16_t) 5 = 16 5 = 80 位升级每当我们要将一个新元素添加到整数集合里面， 并且新元素的类型比整数集合现有所有元素的类型都要长时， 整数集合需要先进行升级（upgrade）， 然后才能将新元素添加到整数集合里面，升级整数集合并添加新元素共分为三步进行 根据新元素的类型， 扩展整数集合底层数组的空间大小， 并为新元素分配空间 将底层数组现有的所有元素都转换成与新元素相同的类型， 并将类型转换后的元素放置到正确的位上， 而且在放置元素的过程中， 需要继续维持底层数组的有序性质不变 将新元素添加到底层数组里面 升级之后新元素的摆放位置，因为引发升级的新元素的长度总是比整数集合现有的所有元素的长度都大，所以这个新元素的值要么就大于所有现有元素，要么就小于所有现有元素 在新元素小于所有现有元素的情况下，新元素会被放置在底层数组的最开头(index=0) 在新元素大于所有现有元素的情况下，新元素会被放置在底层数组的最末尾(index=length-1) 降级整数集合不支持降级操作， 一旦对数组进行了升级， 编码就会一直保持升级后的状态 压缩列表压缩列表(ziplist)是列表键和哈希键的底层实现之一，当一个列表键只包含少量的列表项，并且每个列表项要么就是小整数值，要么就是长度比较短的字符串，redis就会使用压缩列表来做列表键的底层实现 压缩列表的构成压缩列表是 Redis 为了节约内存而开发的， 由一系列特殊编码的连续内存块组成的顺序型（sequential）数据结构 结构如上图 压缩列表节点的构成每个压缩列表节点可以保存一个字节数组或者一个整数值， 其中， 字节数组可以是以下三种长度的其中一种 长度小于等于 63 （2^{6}-1）字节的字节数组 长度小于等于 16383 （2^{14}-1） 字节的字节数组 长度小于等于 4294967295 （2^{32}-1）字节的字节数组 而整数值则可以是以下六种长度的其中一种 4 位长，介于 0 至 12 之间的无符号整数 1 字节长的有符号整数 3 字节长的有符号整数 int16_t 类型整数 int32_t 类型整数 int64_t 类型整数 每个压缩列表节点都由 previous_entry_length 、 encoding 、 content 三个部分组成 节点的 previous_entry_length 属性以字节为单位， 记录了压缩列表中前一个节点的长度 123如果前一节点的长度小于 254 字节， 那么 previous_entry_length 属性的长度为 1 字节： 前一节点的长度就保存在这一个字节里面如果前一节点的长度大于等于 254 字节， 那么 previous_entry_length 属性的长度为 5 字节： 其中属性的第一字节会被设置为 0xFE （十进制值 254）， 而之后的四个字节则用于保存前一节点的长度压缩列表的从表尾向表头遍历操作就是使用这一原理实现的： 只要我们拥有了一个指向某个节点起始地址的指针， 那么通过这个指针以及这个节点的 previous_entry_length 属性， 程序就可以一直向前一个节点回溯， 最终到达压缩列表的表头节点 节点的 encoding 属性记录了节点的 content 属性所保存数据的类型以及长度 12一字节、两字节或者五字节长， 值的最高位为 00 、 01 或者 10 的是字节数组编码： 这种编码表示节点的 content 属性保存着字节数组， 数组的长度由编码除去最高两位之后的其他位记录一字节长， 值的最高位以 11 开头的是整数编码： 这种编码表示节点的 content 属性保存着整数值， 整数值的类型和长度由编码除去最高两位之后的其他位记录 节点的 content 属性负责保存节点的值， 节点值可以是一个字节数组或者整数， 值的类型和长度由节点的 encoding 属性决定 连锁更新考虑一种情况就是，一个列表里全是介于某个临界值的且连续多个的节点，如果在表头插入一个大于这个节点的值，那么节点中的previous_entry_length就会发生大批量的更改来达到效果，同理，小于这个临界值也会发生同样的效果，这就叫连锁更新，因为连锁更新在最坏情况下需要对压缩列表执行 N 次空间重分配操作， 而每次空间重分配的最坏复杂度为 O(N) ， 所以连锁更新的最坏复杂度为 O(N^2)要注意的是， 尽管连锁更新的复杂度较高， 但它真正造成性能问题的几率是很低的 压缩列表里要恰好有多个连续的、长度介于 250 字节至 253 字节之间的节点， 连锁更新才有可能被引发， 在实际中， 这种情况并不多见 即使出现连锁更新， 但只要被更新的节点数量不多， 就不会对性能造成任何影响： 比如说， 对三五个节点进行连锁更新是绝对不会影响性能的 以上原因， ziplistPush 等命令的平均复杂度仅为 O(N) ， 在实际中， 我们可以放心地使用这些函数， 而不必担心连锁更新会影响压缩列表的性能 至此redis的数据结构就已经完事儿了，接下来会复习redis中的二进制位数组（据我了解这个bit是一个很有实用场景的东西，尤其在节约内存方面）","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://grooes.com/tags/redis/"}]},{"title":"PHP中的Traits详解","slug":"PHP中的Traits详解","date":"2017-09-20T13:14:08.000Z","updated":"2017-09-20T13:23:35.157Z","comments":true,"path":"2017/09/20/PHP中的Traits详解/","link":"","permalink":"http://grooes.com/2017/09/20/PHP中的Traits详解/","excerpt":"","text":"原文地址：PHP-Trait特性详解PHP是单继承的语言，在PHP 5.4 Traits出现之前，PHP的类无法同时从两个基类继承属性或方法。php的Traits和Go语言的组合功能类似，通过在类中使用use关键字声明要组合的Trait名称，而具体某个Trait的声明使用trait关键词，Trait不能直接实例化。具体用法请看下面的代码123456789101112131415161718192021222324252627&lt;?php trait Drive &#123; public $carName = 'trait'; public function driving() &#123; echo \"driving &#123;$this-&gt;carName&#125;\\n\"; &#125; &#125; class Person &#123; public function eat() &#123; echo \"eat\\n\"; &#125; &#125; class Student extends Person &#123; use Drive; public function study() &#123; echo \"study\\n\"; &#125; &#125; $student = new Student(); $student-&gt;study(); $student-&gt;eat(); $student-&gt;driving(); //输出结果如下 study eat driving trait 上面的例子中，Student类通过继承Person，有了eat方法，通过组合Drive，有了driving方法和属性carName。 如果Trait、基类和本类中都存在某个同名的属性或者方法，最终会保留哪一个呢？通过下面的代码测试一下：123456789101112131415161718192021222324252627282930&lt;?php trait Drive &#123; public function hello() &#123; echo \"hello drive\\n\"; &#125; public function driving() &#123; echo \"driving from drive\\n\"; &#125; &#125; class Person &#123; public function hello() &#123; echo \"hello person\\n\"; &#125; public function driving() &#123; echo \"driving from person\\n\"; &#125; &#125; class Student extends Person &#123; use Drive; public function hello() &#123; echo \"hello student\\n\"; &#125; &#125; $student = new Student(); $student-&gt;hello(); $student-&gt;driving(); //输出结果如下 hello student driving from drive 因此得出结论：当方法或属性同名时，当前类中的方法会覆盖 trait的 方法，而 trait 的方法又覆盖了基类中的方法。如果要组合多个Trait，通过逗号分隔 Trait名称：1use Trait1, Trait2; 如果多个Trait中包含同名方法或者属性时，会怎样呢？答案是当组合的多个Trait包含同名属性或者方法时，需要明确声明解决冲突，否则会产生一个致命错误。 1234567891011121314151617181920212223&lt;?phptrait Trait1 &#123; public function hello() &#123; echo \"Trait1::hello\\n\"; &#125; public function hi() &#123; echo \"Trait1::hi\\n\"; &#125;&#125;trait Trait2 &#123; public function hello() &#123; echo \"Trait2::hello\\n\"; &#125; public function hi() &#123; echo \"Trait2::hi\\n\"; &#125;&#125;class Class1 &#123; use Trait1, Trait2;&#125;//输出结果如下PHP Fatal error: Trait method hello has not been applied, because there are collisions with other trait methods on Class1 in ~/php54/trait_3.php on line 20 使用insteadof和as操作符来解决冲突，insteadof是使用某个方法替代另一个，而as是给方法取一个别名，具体用法请看代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;?phptrait Trait1 &#123; public function hello() &#123; echo \"Trait1::hello\\n\"; &#125; public function hi() &#123; echo \"Trait1::hi\\n\"; &#125;&#125;trait Trait2 &#123; public function hello() &#123; echo \"Trait2::hello\\n\"; &#125; public function hi() &#123; echo \"Trait2::hi\\n\"; &#125;&#125;class Class1 &#123; use Trait1, Trait2 &#123; Trait2::hello insteadof Trait1; Trait1::hi insteadof Trait2; &#125;&#125;class Class2 &#123; use Trait1, Trait2 &#123; Trait2::hello insteadof Trait1; Trait1::hi insteadof Trait2; Trait2::hi as hei; Trait1::hello as hehe; &#125;&#125;$Obj1 = new Class1();$Obj1-&gt;hello();$Obj1-&gt;hi();echo \"\\n\";$Obj2 = new Class2();$Obj2-&gt;hello();$Obj2-&gt;hi();$Obj2-&gt;hei();$Obj2-&gt;hehe();//输出结果如下：Trait2::helloTrait1::hiTrait2::helloTrait1::hiTrait2::hiTrait1::hello as关键词还有另外一个用途，那就是修改方法的访问控制：123456789101112131415161718192021&lt;?php trait Hello &#123; public function hello() &#123; echo \"hello,trait\\n\"; &#125; &#125; class Class1 &#123; use Hello &#123; hello as protected; &#125; &#125; class Class2 &#123; use Hello &#123; Hello::hello as private hi; &#125; &#125; $Obj1 = new Class1(); $Obj1-&gt;hello(); # 报致命错误，因为hello方法被修改成受保护的 $Obj2 = new Class2(); $Obj2-&gt;hello(); # 原来的hello方法仍然是公共的 $Obj2-&gt;hi(); # 报致命错误，因为别名hi方法被修改成私有的 Trait 也能组合Trait，Trait中支持抽象方法、静态属性及静态方法，测试代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142&lt;?phptrait Hello &#123; public function sayHello() &#123; echo \"Hello\\n\"; &#125;&#125;trait World &#123; use Hello; public function sayWorld() &#123; echo \"World\\n\"; &#125; abstract public function getWorld(); public function inc() &#123; static $c = 0; $c = $c + 1; echo \"$c\\n\"; &#125; public static function doSomething() &#123; echo \"Doing something\\n\"; &#125;&#125;class HelloWorld &#123; use World; public function getWorld() &#123; return 'get World'; &#125;&#125;$Obj = new HelloWorld();$Obj-&gt;sayHello();$Obj-&gt;sayWorld();echo $Obj-&gt;getWorld() . \"\\n\";HelloWorld::doSomething();$Obj-&gt;inc();$Obj-&gt;inc();//输出结果如下：HelloWorldget WorldDoing something12","categories":[],"tags":[{"name":"php","slug":"php","permalink":"http://grooes.com/tags/php/"}]},{"title":"PHP中的浅复制和深复制","slug":"PHP中的浅复制和深复制","date":"2017-09-20T13:03:31.000Z","updated":"2017-09-20T13:04:54.276Z","comments":true,"path":"2017/09/20/PHP中的浅复制和深复制/","link":"","permalink":"http://grooes.com/2017/09/20/PHP中的浅复制和深复制/","excerpt":"","text":"深刻理解PHP中的浅复制和深复制 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;?php/** * 深复制的原理是A的改变不会影响B的改变 * 浅复制的原理是A的改变会影响B的改变**/class ObjA&#123; public $num = 0; public $objB; //包含的对象 function __construct() &#123; $this-&gt;objB = new ObjB(); &#125; //只有实现了下面方法聚合类 才能实现深复制 /*function __clone() &#123; $this-&gt;objB = clone $this-&gt;objB; &#125;*/&#125;class ObjB&#123; public $num2 = 0;&#125;//原型对象$objA = new ObjA();//复制对象（=复制引用）$objA2 = $objA;$objA2-&gt;num = 2;//随着$objA2-&gt;num的变化 $objA-&gt;num也变化了print_r($objA-&gt;num . '&lt;br/&gt;'); //结果为2print_r($objA2-&gt;num . '&lt;br/&gt;'); //结果为2//复制对象（‘clone’关键字克隆）$objA3 = clone $objA;$objA3-&gt;num = 4;//随着$objA3-&gt;num的变化 $objA-&gt;num没有变化print_r($objA-&gt;num . '&lt;br/&gt;'); //结果为2print_r($objA3-&gt;num . '&lt;br/&gt;'); //结果为4//但是clone的对象（是聚合类）中包含其他对象时所包含的对象（objB）复制的是引用$objA3-&gt;objB-&gt;num2 = 7;print_r($objA3-&gt;objB-&gt;num2 . '&lt;br/&gt;'); //结果是7print_r($objA-&gt;objB-&gt;num2 . '&lt;br/&gt;'); //结果是7","categories":[],"tags":[{"name":"php","slug":"php","permalink":"http://grooes.com/tags/php/"}]},{"title":"实现一个base62_encode和base62_decode方法","slug":"实现一个base62-encode和base62-decode方法","date":"2017-09-20T12:33:51.000Z","updated":"2017-09-20T12:35:42.246Z","comments":true,"path":"2017/09/20/实现一个base62-encode和base62-decode方法/","link":"","permalink":"http://grooes.com/2017/09/20/实现一个base62-encode和base62-decode方法/","excerpt":"","text":"实现一个base62_encode()和base62_decode()方法，要求base62_encode(1)=1,base62_encode(61)=z,base62_decode(‘z’)=61;语言不限 1234567891011121314151617181920212223242526272829303132333435&lt;?php//如果有经验的RD，一眼就知道62个字符是0-9A-Za-z，并且在微博推出的短链服务就知道，这是一道通向短链设计的一个必经之路，具体短链服务不懂怎么设计的自行google，这里不做过多解释class Base62&#123; private $string = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"; public function base62_encode($str) &#123; $out = ''; for ($t = floor(log10($str) / log10(62)); $t &gt;= 0; $t--) &#123; $a = floor($str / pow(62, $t)); $out = $out . substr($this-&gt;string, $a, 1); $str = $str - ($a * pow(62, $t)); &#125; return $out; &#125; public function base62_decode($str) &#123; $out = 0; $len = strlen($str) - 1; for ($t = 0; $t &lt;= $len; $t++) &#123; $out = $out + strpos($this-&gt;string, substr($str, $t, 1)) * pow(62, $len - $t); &#125; return substr(sprintf(\"%f\", $out) , 0, -7); &#125;&#125;$object = new Base62();echo $object-&gt;base62_encode(1) . \"&lt;br/&gt;\";echo $object-&gt;base62_encode(61) . \"&lt;br/&gt;\";echo $object-&gt;base62_decode('z') . \"&lt;br/&gt;\"; ?&gt;","categories":[],"tags":[{"name":"php","slug":"php","permalink":"http://grooes.com/tags/php/"},{"name":"interview","slug":"interview","permalink":"http://grooes.com/tags/interview/"},{"name":"algorithm","slug":"algorithm","permalink":"http://grooes.com/tags/algorithm/"}]},{"title":"go语言实现排序算法","slug":"go语言实现排序算法","date":"2017-09-20T12:01:51.000Z","updated":"2017-09-20T12:03:09.177Z","comments":true,"path":"2017/09/20/go语言实现排序算法/","link":"","permalink":"http://grooes.com/2017/09/20/go语言实现排序算法/","excerpt":"","text":"Go实现相关排序算法 Go实现冒泡排序 1234567891011121314151617181920212223242526272829303132333435package mainimport \"fmt\"func main() &#123; var arr = []int&#123;9,8,7,5,3,4,6,2,1,3,0&#125; bubbleAsort(arr) bubbleZsort(arr)&#125;//倒序func bubbleAsort(arr []int) &#123; for i :=0; i &lt; len(arr)-1; i++ &#123; for j := i+1; j&lt; len(arr); j++ &#123; if (arr[i] &lt; arr[j]) &#123; arr[i],arr[j] = arr[j],arr[i] &#125; &#125; &#125; fmt.Println(arr)&#125;//正序func bubbleZsort(arr []int) &#123; for i :=0; i &lt; len(arr)-1; i++ &#123; for j := i+1; j&lt; len(arr); j++ &#123; if (arr[i] &gt; arr[j]) &#123; arr[i],arr[j] = arr[j],arr[i] &#125; &#125; &#125; fmt.Println(arr)&#125; Go实现选择排序 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package mainimport \"fmt\"func main() &#123; var arr = []int&#123;9,8,7,5,3,4,6,2,1,3,0&#125; selectAsort(arr) selectZsort(arr)&#125;//倒序func selectAsort(arr []int) &#123; l := len(arr) m := len(arr) - 1 for i := 0; i &lt; m; i++ &#123; k := i for j := i+1; j &lt; l; j++ &#123; if arr[k] &lt; arr[j] &#123; k = j &#125; &#125; if k != i &#123; arr[k],arr[i] = arr[i],arr[k] &#125; &#125; fmt.Println(arr)&#125;//正序func selectZsort(arr []int) &#123; l := len(arr) m := len(arr) - 1 for i := 0; i &lt; m; i++ &#123; k := i for j := i+1; j &lt; l; j++ &#123; if arr[k] &gt; arr[j] &#123; k = j &#125; &#125; if k != i &#123; arr[k],arr[i] = arr[i],arr[k] &#125; &#125; fmt.Println(arr)&#125; Go实现快速排序 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package mainimport \"fmt\"func main() &#123; var arr = []int&#123;9,8,7,5,3,4,6,2,1,0&#125; quickAsort(arr, 0, len(arr)-1) fmt.Println(arr) quickZsort(arr, 0, len(arr)-1) fmt.Println(arr)&#125;//倒序func quickAsort(arr []int, start, end int) &#123; if (start &lt; end) &#123; i, j := start, end key := arr[(start + end)/2] for i &lt;= j &#123; for arr[i] &gt; key &#123; i++ &#125; for arr[j] &lt; key &#123; j-- &#125; if i &lt;= j &#123; arr[i], arr[j] = arr[j], arr[i] i++ j-- &#125; &#125; if start &lt; j &#123; quickAsort(arr, start, j) &#125; if end &gt; i &#123; quickAsort(arr, i, end) &#125; &#125;&#125;//正序func quickZsort(arr []int, start, end int) &#123; if (start &lt; end) &#123; i, j := start, end key := arr[(start + end)/2] for i &lt;= j &#123; for arr[i] &lt; key &#123; i++ &#125; for arr[j] &gt; key &#123; j-- &#125; if i &lt;= j &#123; arr[i], arr[j] = arr[j], arr[i] i++ j-- &#125; &#125; if start &lt; j &#123; quickZsort(arr, start, j) &#125; if end &gt; i &#123; quickZsort(arr, i, end) &#125; &#125;&#125; Go实现插入排序 123456789101112131415161718192021package mainimport \"fmt\"func main() &#123; var arr = []int&#123;9,8,7,5,3,4,6,2,1,0&#125; insertSort(arr) fmt.Println(arr)&#125;func insertSort(arr []int) &#123; n := len(arr) if n &lt; 2 &#123; return &#125; for i := 1; i &lt; n; i++ &#123; for j := i; j &gt;0 &amp;&amp; arr[j] &lt; arr[j-1]; j-- &#123; arr[j], arr[j-1] = arr[j-1], arr[j] &#125; &#125;&#125; Go实现希尔排序 12345678910111213141516171819202122232425262728package mainimport \"fmt\"func main() &#123; var arr = []int&#123;9,8,7,5,3,4,6,2,1,0&#125; shellSort(arr) fmt.Println(arr)&#125;func shellSort(arr []int) &#123; n := len(arr) h := 1 //寻找合适的间隔h for h &lt; n/3 &#123; h = 3*h +1 &#125; for h &gt;= 1 &#123; for i := h; i &lt; n; i++ &#123; for j := i; j &gt;= h &amp;&amp; arr[j] &lt; arr[j-1]; j -= h &#123; arr[j], arr[j-1] = arr[j-1], arr[j] &#125; &#125; h /= 3 &#125;&#125;","categories":[],"tags":[{"name":"go","slug":"go","permalink":"http://grooes.com/tags/go/"}]},{"title":"Go语言中的 Array, Slice和 Map","slug":"Go语言中的-Array-Slice和-Map","date":"2017-09-17T12:51:23.000Z","updated":"2017-09-17T13:31:38.101Z","comments":true,"path":"2017/09/17/Go语言中的-Array-Slice和-Map/","link":"","permalink":"http://grooes.com/2017/09/17/Go语言中的-Array-Slice和-Map/","excerpt":"","text":"Array, Slice和 Map的区别 Array 是值类型，Slice 和 Map 是引用类型。他们是有很大区别的，尤其是在参数传递的时候。 Slice 和 Map 的变量 仅仅声明是不行的，必须还要分配空间（也就是初始化，initialization） 才可以使用。 Slice 和 Map 这些引用变量 的 内存分配，不需要你操心，因为 golang 是存在 gc 机制的（垃圾回收机制） Array 的用法 数组的声明（这里就是定义，给数据存储分配了空间） 1var arrayName [arraySize] dataType 如果数组定义好之后， 没有给数组元素指定值，那么所有元素被自动初始化为零值 数组的初始化 12345var a = [10]int&#123;1,2,3,4,5,6,7,8,9,10&#125; //定义数组的时候，直接初始化var b = [10]int &#123;1, 2, 3, 4&#125; //部分元素初始化， 其余元素零值var c = [...]int &#123;1, 2, 3, 4, 5&#125; //由初始化列表决定数组长度，不可省去标识符 \"...\"，否则将变成切片Slicevar d = [10]&#123;2:4, 5:7&#125; //可以按照下标来进行初始化 数组的访问，可以直接按照下标进行访问 数组的遍历(for) 12345678910111213141516package mainimport( \"fmt\")func main() &#123; var f = [20]int &#123;1, 1&#125; for i := 2; i &lt; 20; i++ &#123; f[i] = f[i-1] + f[i-2] &#125; for i := 0; i &lt; 20; i++ &#123; //采用下标进行遍历 if i % 5 == 0 &#123; fmt.Printf(\"\\n\") &#125; fmt.Printf(\"f[%2d] = %4d\",i , f[i]) &#125;&#125; 数组的遍历(range) 12345678910111213package mainimport( \"fmt\")func main() &#123; var f = [20]int &#123;1, 1&#125; for i := 2; i &lt; 20; i++ &#123; f[i] = f[i-1] + f[i -2] &#125; for i , v := range f &#123; //采用 range 关键字 进行遍历 fmt.Printf(\"f[%2d] = %4d\", i, v) &#125;&#125; 多维数组 12var a [3][4]intvar b = [3][4]int &#123;&#123;1,2&#125;, &#123;1,2,3,4&#125;, &#123;2,3, 4&#125;&#125; 多维数组遍历 123456789101112131415161718package mainimport \"fmt\"func main() &#123; //找到二维数组中的最大元素 var i, j, row, col, max int var a = [3][4]int &#123;&#123;1, 3, 7, 3&#125;, &#123;2, 3, 7 , 9&#125;, &#123;22, 3, 5, 10&#125;&#125; max = a[0][0] for i := 0; i &lt; = 2; i ++ &#123; for j := 0; j &lt;= 3; j++ &#123; if a[i][j] &gt; max &#123; max = a[i][j] row = i col = j &#125; &#125; &#125; fmt.Println(\"max = %d, row = %d, col = %d\\n\", max, row, col)&#125; Slice 的用法 Slice 的声明（没有分配内存） 1var s1 []int 在创建切片的时候，不要指定切片的长度。（否则就成了数组） 切片的类型可以是Go 语言的任何基本数据类型（也包括 引用类型和 Struct 类型） 当一个切片被声明之后，没有初始化的时候，这个 s1 默认的值是 nil。切片的长度是0。可以使用内建函数 len() 获得切片的长度，使用内建函数 cap() 获得切片的容量。 Slice 的创建 (分配了内存) 三种创建方式： 基于底层数组创建，直接创建，或者 make() 函数创建 基于底层数组创建 slice 123456var slice1 []int //声明但是不分配空间slice1 = array[start:end] //这里不包含 endslice2 := array[:] // 引用全部的元素slice3 := array[0:len(array)]var slice4 []intsliec34 = array //引用全部的元素 直接创建 slice(在声明的时候，直接初始化。) 1var slice1 = []int &#123;1 ,2, 3, 4, 5&#125; make() 函数创建 slice 12var slice1 = make([]int, 5) //长度和容量都是 5var slice2 = make([]int, 5, 10) //容量是5. Slice 的 访问和遍历(采用下标进行访问，采用 range 进行遍历。) 1234567891011121314packge mainimport \"fmt\"func main() &#123; var slice1 = []int &#123;1, 2,3 , 4, 5&#125; //使用下标访问 slice for i := 0; i &lt;=4; i++ &#123; fmt.Println(\"slice[%d] = %d\", i, slice[i]) &#125; fmt.Println() //使用range 进行遍历 for i, v := range slice &#123; fmt.Println(\"slice[%d] = %d\", i, v) &#125; &#125; Slice 的操作 Slice 中的切片的元素，可以动态的添加和删除，所以操作起来要比数组更加方便 采用内建函数 append() 向切片尾部，增加新的元素， 这些元素保存到底层的数组。 append() 并不会影响原来的切片的属性，（原来切片的长度和cap） append() 将会返回更新后的切片的对象。 append() 是个变参函数，可以一次性添加多个对象 append() 添加元素的个数超过 切片的 cap() 的时候，那么底层会 重新分配一个 “足够大” 的内存，一般来说是将原来的内存空间扩大二倍，然后将数据复制到新的内存中去， 原来的空间会保留 （供原先切片使用）（底层数组变化这个问题，应该关注一下） 12345678910111213141516171819202122package mainimport \"fmt\"func main() &#123; //使用make 创建 切片 var slice1 = make([]int, 3, 6) // 使用 append 添加元素，并且未超出 cap slice2 := append(slice1, 1, 2, 3) // 使用 append 添加元素，并且超出 cap. 这个时候底层数组会变化，新增加的元素只会添加到新的底层数组，不会覆盖旧的底层数组。 slice3 := append(slice1, 4, 5, 6, 7) slice1[0] = 10 fmt.Printf(\"len = %d cap = %d %v\\n\", len(slice1), cap(slice1), slice1) fmt.Printf(\"len = %d cap = %d %v\\n\", len(slice2), cap(slice2), slice2) fmt.Printf(\"len = %d cap = %d %v\\n\", len(slice3), cap(slice3), slice3)&#125;程序输出是：len = 3 cap = 6 [10 0 0]len = 6 cap = 6 [10 0 0 1 2 3] // 这里的[1, 2, 3] 没有被 [4, 5, 6]覆盖len = 7 cap = 12 [0 0 0 4 5 6 7] //这里第一个元素没有变成10，并且容量变成原来的2倍。 切片元素的复制 使用切片长时间引用超大的底层数组，会导致严重的内存浪费现象。 可以新建一个小的slice 对象，然后将所需要的数据复制过去，这样子就不会引用底层数组，直接拷贝了数据，这就是需求。函数 copy()可以 在切片之间复制元素。 copy() 可以复制的元素数量取决于 复制方 和 被复制方的最小长度。 同一个底层数组之间的 元素复制，会导致元素重叠问题。12345678910111213141516171819package mainimport \"fmt\"func main() &#123; var slice1 = []int&#123;1, 2, 3, 4, 5, 6, 7, 8, 9, 10&#125; var slice2 = make([]int, 3, 5) var n int n = copy(slice2, slice1) // just copy three elements fmt.Println(n, slice2, len(slice2), cap(slice2)) slice3 := slice1[3:6] //二者引用同一个底层数组 n = copy(slice3, slice1[1:5]) //所以，copy的时候发生元素重叠 fmt.Println(n, slice1, slice3)&#125;程序输出为：3 [1 2 3] 3 53 [1 2 3 2 3 4 7 8 9 10] [2 3 4] //可以看到元素重叠 Map 的用法map 存储的是 键值对(key-value)。是一个无序的数据的集合，通过键来进行索引得到对应的值。 这种方式可以加快查找速度。Map 通常称为 字典（dictionary） 或者哈希表(Hash table)。Map 现在是很多语言的标配。 字典的声明 字典名称，“键”类型， “值”类型 不需要给字典指定长度，字典的长度会在初始化或者创建的过程中动态增长 Key 必须是能支持 比较运算符（==, !=）的数据类型，比如 整数，浮点数，指针，数组，结构体，接口等。 而不能是 函数，字典，切片这些类型。 Value 类型 可以是Go语言的任何基本数据类型。1var mapName map[keyType]valueType 字典的初始化 和 创建 字典 声明好之后，必须经过初始化或者创建 才能使用。未初始化或者创建的字典为 nil 可以使用“{}”来在声明的时候进行初始化 可是使用 make()来创建字典 创建或者初始化之后，就可以使用 “=”操作符来动态的向字典中添加数据项了1234567891011var map1 map[string]int &#123;&#125; //字典的初始化map1[\"key1\"] = 1var map2 map[string]intmap2 = make(map[string]int) //字典的创建map2[\"key2\"] = 2 //使用 等号 添加数据项var map3 map[string]intmap3[\"key1\"] = 2 //编译不通过，字典没有初始化或者创建v, OK := mapName[Key] //元素的查找 注意这里是 := 12345678910111213141516171819202122232425package mainimport \"fmt\"func main() &#123; var map1 = map[string]int&#123;\"key1\": 100, \"key2\": 200&#125; // v, OK := map1[\"key1\"] if OK &#123; fmt.Println(v, OK) &#125; else &#123; fmt.Println(v) &#125; // 这里 不是 :=，是 = ，因为这些变量已经定义过了。 v, OK = map1[\"key3\"] if OK &#123; fmt.Println(v, OK) &#125; else &#123; fmt.Println(v) &#125;&#125;输出为：100 true0 字典项的删除 go 提供了内置函数 delete() 来删除容器内的元素。(如果key1值不存在，那么调用将什么也不发生，也不会产生副作用。 但是，如果传入的map 是一个 nil，那么将导致程序出现异常，这一点在写程序的时候特别注意。)1delete(map1, \"key1\") 12345678910111213141516171819202122232425package mainimport ( \"fmt\")func main() &#123; var map1 = map[string]int&#123;\"key1\": 100, \"key2\": 200, \"key3\": 300&#125; for k, v := range map1 &#123; fmt.Println(k, v) if k == \"key2\" &#123; delete(map1, k) &#125; if k == \"key3\" &#123; map1[\"key4\"] = 400 &#125; &#125; fmt.Println(map1)&#125;程序输出：key2 200key3 300key1 100map[key1:100 key4:400 key3:300] //可以看到 map 是无序的。","categories":[],"tags":[{"name":"go","slug":"go","permalink":"http://grooes.com/tags/go/"}]},{"title":"mysql安装那点事儿","slug":"mysql安装那点事儿","date":"2017-09-17T09:02:24.000Z","updated":"2017-09-17T09:03:35.251Z","comments":true,"path":"2017/09/17/mysql安装那点事儿/","link":"","permalink":"http://grooes.com/2017/09/17/mysql安装那点事儿/","excerpt":"","text":"安装mysql rpm -qa | grep mysql // 这个命令就会查看该操作系统上是否已经安装了mysql数据库rpm -e mysql // 普通删除模式rpm -e –nodeps mysql // 强力删除模式，如果使用上面命令删除时，提示有依赖的其它文件，则用该命令可以对其进行强力删除yum list | grep mysql //查看yum上提供的mysql数据库可下载的版本yum install -y mysql-server mysql mysql-deve //下载rpm -qi mysql-server //查看mysql版本service mysqld start //启动mysqlservice mysqld restart //重启mysqlchkconfig –list | grep mysqld //查看mysql服务是不是开机自动启动mysqld 0:关闭 1:关闭 2:关闭 3:关闭 4:关闭 5:关闭 6:关闭chkconfig mysqld on //设置成开机启动，这样就不用每次都去手动启动了/usr/bin/mysqladmin -u root password ‘root’ //设置账号密码mysql -u root -p //进入mysql数据库Enter password: //输入密码即可完毕！ centos6.6安装mysql5.7.17 因用yum安装的是比较古老的mysql版本，所以需要升级最新版本首先查看安装的mysql，需要全部移除rpm -qa | grep mysqlmysql-community-common-5.1.17-1.el6.x86_64mysql-community-libs-5.2.17-1.el6.x86_64mysql-community-server-5.1.17-1.el6.x86_64mysql-community-client-5.1.17-1.el6.x86_64一共四个需要全部删除rpm -e –nodeps mysql-community-common-5.1.17-1.el6.x86_64rpm -e –nodeps mysql-community-libs-5.2.17-1.el6.x86_64rpm -e –nodeps mysql-community-server-5.1.17-1.el6.x86_64rpm -e –nodeps mysql-community-client-5.1.17-1.el6.x86_64rpm -qa | grep mysql确保没有了mysql 进入：https://dev.mysql.com/downloads/file/?id=467446下载到的包：mysql-5.7.17-1.el6.x86_64.rpm-bundle.tarcd /tmprz mysql-5.7.17-1.el6.x86_64.rpm-bundle.tartar -xvf mysql-5.7.17-1.el6.x86_64.rpm-bundle.tarrpm -ivh mysql-community-common-5.7.17-1.el6.x86_64.rpmrpm -ivh mysql-community-libs-5.7.17-1.el6.x86_64.rpmrpm -ivh mysql-community-client-5.7.17-1.el6.x86_64.rpmrpm -ivh mysql-community-server-5.7.17-1.el6.x86_64.rpmchmod 777 -R /var/lib/mysqlservice mysqld startMySQL Daemon failed to start.正在启动 mysqld： [失败]getenforceEnforcingsetenforce 0service mysqld start正在启动 mysqld： [确定]mysqlERROR 1045 (28000): Access denied for user ‘root’@’localhost’ (using password: NO)尼玛，我怎么知道密码？我都没设置密码shell&gt;/usr/bin/mysqld_safe –skip-grant-tables &amp;shell&gt;mysql -u root -p下面的密码直接键入回车即可.mysql&gt;use mysqlmysql&gt;update mysql.user set authentication_string=PASSWORD(‘root’) where user=’root’ and host=’localhost’;mysql&gt;flush privileges;mysql&gt;exit;shell&gt;mysql -u root -p输入密码即可完毕！突然发现在关闭服务器后去启动mysql的时候又启动失败了猜想是getenforce，于是乎执行getenforce竟然成了Enforcing，如果每次启动都这样，岂不是很累，于是乎不能忍google了下，永久解决办法是设置永久性的shell&gt;vi /etc/sysconfig/selinuxSELINUX=disabled 设置成这样既可，打开是知道共有三个值可以选择的done!然后进入shell&gt;mysql -u root -p输入密码后执行mysql&gt;show databases;ERROR 1820 (HY000): You must reset your password using ALTER USER statement before executing this statement.错误解决办法：mysql&gt;SET PASSWORD= PASSWORD(‘new password’);mysql&gt;ALTER USER ‘root’@’localhost’ PASSWORD EXPIRE NEVER;mysql&gt;flush privileges;mysql&gt;exit;重新登录既可done!","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"http://grooes.com/tags/linux/"},{"name":"mysql","slug":"mysql","permalink":"http://grooes.com/tags/mysql/"}]},{"title":"PHP7环境搭建那点事儿","slug":"PHP7环境搭建那点事儿","date":"2017-09-17T01:30:09.000Z","updated":"2017-09-17T02:16:26.950Z","comments":true,"path":"2017/09/17/PHP7环境搭建那点事儿/","link":"","permalink":"http://grooes.com/2017/09/17/PHP7环境搭建那点事儿/","excerpt":"","text":"安装包准备 nginx-1.11.6.tar.gz google-perftools-1.6.tar.gz libevent-2.0.22-stable.tar.gz libiconv-1.13.1.tar.gz libmcrypt-2.5.8.tar.gz libmemcached-1.0.18.tar.gz m9php-php7.tar.gz mcrypt-2.6.8.tar.gz memcache-3.0.8.tgz memcached-1.4.34.tar.gz mhash-0.9.9.9.tar.gz pecl-memcache-php7.tar.gz php-7.0.14.tar.gz php-memcached-master.tar.gz phpredis-develop.tar.gz protobuf-master.tar.gz 环境准备 yum -y install lrzsz（为了运用rz命令把本地包上次上去） yum -y install wget(为了下载包) yum -y install zip unzip(为了解压包) yum -y install gcc(安装gcc) yum -y install openssl openssl-devel(安装openssl) yum -y install pcre*(安装pcre) yum -y install gcc gcc-c++(安装c++) yum -y install libxml2(安装相关libxml2) yum -y install curl(安装curl相关) yum -y install curl-devel(安装curl-devel) yum -y install libpng*(安装libpng相关) 相关环境搭建 安装nginx tar zxf nginx-1.11.6.tar.gzcd nginx-1.11.6./configure –prefix=/usr/local/nginx –user=www –group=www –with-http_stub_status_module –with-http_flv_module –with-http_ssl_modulemake &amp;&amp; make install 启动nginx /usr/local/nginx/sbin/nginx报错：nginx: [emerg] getpwnam(“www”) failed解决方式1：在nginx.conf中 把user nobody的注释去掉既可解决方式2：/usr/sbin/groupadd -f www /usr/sbin/useradd -g www www 本机浏览器输入：http://ip/成功标识：Welcome to nginx!失败标识：考虑关闭防火墙或者打开80端口即可 安装libiconv tar zxf libiconv-1.13.1.tar.gzcd libiconv-1.13.1/./configure –prefix=/usr/local/libiconvmake &amp;&amp; make install 安装libevent tar zxvf libevent-2.0.22-stable.tar.gz cd libevent-2.0.22-stable./configure –prefix=/usr/local/libeventmake &amp;&amp; make test &amp;&amp; make install 安装libmcrypt tar zxf libmcrypt-2.5.8.tar.gzcd libmcrypt-2.5.8/./configuremake &amp;&amp; make install 安装mhash tar zxf mhash-0.9.9.9.tar.gzcd mhash-0.9.9.9./configuremake &amp;&amp; make install 开启软连接 ln -s /usr/local/lib/libmcrypt.la /usr/lib/libmcrypt.laln -s /usr/local/lib/libmcrypt.so /usr/lib/libmcrypt.soln -s /usr/local/lib/libmcrypt.so.4 /usr/lib/libmcrypt.so.4ln -s /usr/local/lib/libmcrypt.so.4.4.8 /usr/lib/libmcrypt.so.4.4.8ln -s /usr/local/lib/libmhash.a /usr/lib/libmhash.aln -s /usr/local/lib/libmhash.la /usr/lib/libmhash.laln -s /usr/local/lib/libmhash.so /usr/lib/libmhash.soln -s /usr/local/lib/libmhash.so.2 /usr/lib/libmhash.so.2ln -s /usr/local/lib/libmhash.so.2.0.1 /usr/lib/libmhash.so.2.0.1ln -s /usr/local/bin/libmcrypt-config /usr/bin/libmcrypt-config 安装memcached tar -zxvf memcached-1.4.34.tar.gzcd memcached-1.4.34./configure –prefix=/usr/local/memcached –with-libevent=/usr/local/libevent/make &amp;&amp; make install启动memcached :/usr/local/memcached/bin/memcached -d -m 100 -u root -l 127.0.0.1 -p 11211 -c 256 -P /tmp/memcached.pid 安装mcrypt tar zvxf mcrypt-2.6.8.tar.gzcd mcrypt-2.6.8./configure报错：configure: error: * libmcrypt was not found （其实我已经安装了libmcrypt）解决：export LD_LIBRARY_PATH=/usr/local/lib: LD_LIBRARY_PATH./configuremake &amp;&amp; make install 安装php7 tar zvxf php-7.0.14.tar.gzcd php-7.0.14./configure –prefix=/usr/local/php –with-config-file-path=/usr/local/php/etc –enable-fpm –enable-pcntl –enable-mysqlnd –enable-opcache –enable-sockets –enable-sysvmsg –enable-sysvsem –enable-sysvshm –enable-shmop –enable-zip –enable-ftp –enable-soap –enable-xml –enable-mbstring –disable-rpath –disable-debug –disable-fileinfo –with-mysqli –with-pdo-mysql –with-pcre-regex –with-iconv –with-zlib –with-mcrypt –with-gd –with-openssl –with-mhash –with-xmlrpc –with-curl –without-pear –enable-fileinfo –with-imap-sslmake &amp;&amp; make installcp ./php.ini-development /usr/local/php/etc/php.inicp /usr/local/php/etc/php-fpm.conf.default /usr/local/php/etc/php-fpm.confcp /usr/local/php/etc/php-fpm.d/www.conf.default /usr/local/php/etc/php-fpm.d/www.confcp -R ./sapi/fpm/php-fpm /etc/init.d/php-fpm 安装libmemcached tar zxvf libmemcached-1.0.18.tar.gzcd libmemcached-1.0.18./configure –prefix=/usr/local/libmemcached –with-memcachedmake &amp;&amp; make install 安装php-memcached扩展 tar xvzf php-memcached-master.tar.gzcd php-memcached-master/usr/local/php/bin/phpize./configure –enable-memcached –with-php-config=/usr/local/php/bin/php-config –with-libmemcached-dir=/usr/local/libmemcached –disable-memcached-saslmake &amp;&amp; make install查看是否安装成功/usr/local/php/bin/php -m 测试php-memcached扩展 vi /usr/local/php/etc/php.ini添加：extension=memcached.so wq!启动fpm:/usr/local/php/sbin/php-fpm -R验证：ps -ef | grep ‘fpm’启动memcached:/usr/local/memcached/bin/memcached -d -m 100 -u root -l 127.0.0.1 -p 11211 -c 256 -P /tmp/memcached.pid验证：ps -ef | grep ‘memcached’验证是否连接成功vi /tmp/memcached.php输入：12345678910111213&lt;?php $m = new Memcached(); $m-&gt;addServer('127.0.0.1', 11211); $m-&gt;set('int', 99); $m-&gt;set('string', 'a simple string'); $m-&gt;set('array', array(11, 12)); $m-&gt;set('object', new stdclass, time() + 300); var_dump($m-&gt;get('int')); var_dump($m-&gt;get('string')); var_dump($m-&gt;get('array')); var_dump($m-&gt;get('object'));?&gt; wq!cd /tmp执行：/usr/local/php/bin/php memcached.php 打印成功即可 测试php-mysql扩展 vi /tmp/mysql.php1234567&lt;?php $pdo = new PDO(\"mysql:host=hostname;dbname=databasename\",\"root\",\"\"); $rs = $pdo -&gt; query(\"select * from test\"); while($row = $rs -&gt; fetch())&#123; print_r($row); &#125;?&gt; wq!cd /tmp执行：/usr/local/php/bin/php mysql.php 打印成功即可 安装php-redis扩展 tar xvf phpredis-develop.tar.gzcd phpredis-develop/usr/local/php/bin/phpize./configure –with-php-config=/usr/local/php/bin/php-configmake &amp;&amp; make installvi /usr/local/php/etc/php.ini输入：extension=redis.sowq!查看是否安装成功/usr/local/php/bin/php -m 安装m9php-php7扩展 tar -zxvf m9php-php7.tar.gzcd m9php/m9php/usr/local/php/bin/phpize./configure –with-php-config=/usr/local/php/bin/php-configmake &amp;&amp; make installvi /usr/local/php/etc/php.ini输入：extension=m9php.sowq!查看是否安装成功/usr/local/php/bin/php -m","categories":[],"tags":[{"name":"php","slug":"php","permalink":"http://grooes.com/tags/php/"},{"name":"linux","slug":"linux","permalink":"http://grooes.com/tags/linux/"}]}]}